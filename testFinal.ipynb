{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ambar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from spimi import SPIMI\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testeo\n",
    "path = './dataset/'\n",
    "data_path = path + 'spotify_songs_test2.csv'\n",
    "\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./blocks1/block_0.txt',\n",
       " './blocks1/block_1.txt',\n",
       " './blocks1/block_2.txt',\n",
       " './blocks1/block_3.txt',\n",
       " './blocks1/block_4.txt',\n",
       " './blocks1/block_5.txt',\n",
       " './blocks1/block_6.txt',\n",
       " './blocks1/block_7.txt',\n",
       " './blocks1/block_8.txt',\n",
       " './blocks1/block_9.txt',\n",
       " './blocks1/block_10.txt',\n",
       " './blocks1/block_11.txt',\n",
       " './blocks1/block_12.txt',\n",
       " './blocks1/block_13.txt',\n",
       " './blocks1/block_14.txt',\n",
       " './blocks1/block_15.txt',\n",
       " './blocks1/block_16.txt',\n",
       " './blocks1/block_17.txt',\n",
       " './blocks1/block_18.txt',\n",
       " './blocks1/block_19.txt',\n",
       " './blocks1/block_20.txt',\n",
       " './blocks1/block_21.txt',\n",
       " './blocks1/block_22.txt',\n",
       " './blocks1/block_23.txt',\n",
       " './blocks1/block_24.txt',\n",
       " './blocks1/block_25.txt',\n",
       " './blocks1/block_26.txt',\n",
       " './blocks1/block_27.txt',\n",
       " './blocks1/block_28.txt',\n",
       " './blocks1/block_29.txt',\n",
       " './blocks1/block_30.txt',\n",
       " './blocks1/block_31.txt',\n",
       " './blocks1/block_32.txt',\n",
       " './blocks1/block_33.txt',\n",
       " './blocks1/block_34.txt',\n",
       " './blocks1/block_35.txt',\n",
       " './blocks1/block_36.txt',\n",
       " './blocks1/block_37.txt',\n",
       " './blocks1/block_38.txt',\n",
       " './blocks1/block_39.txt',\n",
       " './blocks1/block_40.txt',\n",
       " './blocks1/block_41.txt',\n",
       " './blocks1/block_42.txt',\n",
       " './blocks1/block_43.txt',\n",
       " './blocks1/block_44.txt',\n",
       " './blocks1/block_45.txt',\n",
       " './blocks1/block_46.txt',\n",
       " './blocks1/block_47.txt',\n",
       " './blocks1/block_48.txt',\n",
       " './blocks1/block_49.txt',\n",
       " './blocks1/block_50.txt',\n",
       " './blocks1/block_51.txt',\n",
       " './blocks1/block_52.txt',\n",
       " './blocks1/block_53.txt',\n",
       " './blocks1/block_54.txt',\n",
       " './blocks1/block_55.txt',\n",
       " './blocks1/block_56.txt',\n",
       " './blocks1/block_57.txt',\n",
       " './blocks1/block_58.txt',\n",
       " './blocks1/block_59.txt',\n",
       " './blocks1/block_60.txt',\n",
       " './blocks1/block_61.txt',\n",
       " './blocks1/block_62.txt',\n",
       " './blocks1/block_63.txt',\n",
       " './blocks1/block_64.txt',\n",
       " './blocks1/block_65.txt',\n",
       " './blocks1/block_66.txt',\n",
       " './blocks1/block_67.txt',\n",
       " './blocks1/block_68.txt',\n",
       " './blocks1/block_69.txt',\n",
       " './blocks1/block_70.txt',\n",
       " './blocks1/block_71.txt',\n",
       " './blocks1/block_72.txt',\n",
       " './blocks1/block_73.txt',\n",
       " './blocks1/block_74.txt',\n",
       " './blocks1/block_75.txt',\n",
       " './blocks1/block_76.txt',\n",
       " './blocks1/block_77.txt',\n",
       " './blocks1/block_78.txt',\n",
       " './blocks1/block_79.txt',\n",
       " './blocks1/block_80.txt',\n",
       " './blocks1/block_81.txt',\n",
       " './blocks1/block_82.txt',\n",
       " './blocks1/block_83.txt',\n",
       " './blocks1/block_84.txt',\n",
       " './blocks1/block_85.txt',\n",
       " './blocks1/block_86.txt',\n",
       " './blocks1/block_87.txt',\n",
       " './blocks1/block_88.txt',\n",
       " './blocks1/block_89.txt',\n",
       " './blocks1/block_90.txt',\n",
       " './blocks1/block_91.txt',\n",
       " './blocks1/block_92.txt',\n",
       " './blocks1/block_93.txt',\n",
       " './blocks1/block_94.txt',\n",
       " './blocks1/block_95.txt',\n",
       " './blocks1/block_96.txt',\n",
       " './blocks1/block_97.txt',\n",
       " './blocks1/block_98.txt',\n",
       " './blocks1/block_99.txt',\n",
       " './blocks1/block_100.txt',\n",
       " './blocks1/block_101.txt',\n",
       " './blocks1/block_102.txt',\n",
       " './blocks1/block_103.txt',\n",
       " './blocks1/block_104.txt',\n",
       " './blocks1/block_105.txt',\n",
       " './blocks1/block_106.txt',\n",
       " './blocks1/block_107.txt',\n",
       " './blocks1/block_108.txt',\n",
       " './blocks1/block_109.txt',\n",
       " './blocks1/block_110.txt',\n",
       " './blocks1/block_111.txt',\n",
       " './blocks1/block_112.txt',\n",
       " './blocks1/block_113.txt',\n",
       " './blocks1/block_114.txt',\n",
       " './blocks1/block_115.txt',\n",
       " './blocks1/block_116.txt',\n",
       " './blocks1/block_117.txt',\n",
       " './blocks1/block_118.txt',\n",
       " './blocks1/block_119.txt',\n",
       " './blocks1/block_120.txt',\n",
       " './blocks1/block_121.txt',\n",
       " './blocks1/block_122.txt',\n",
       " './blocks1/block_123.txt',\n",
       " './blocks1/block_124.txt',\n",
       " './blocks1/block_125.txt',\n",
       " './blocks1/block_126.txt',\n",
       " './blocks1/block_127.txt',\n",
       " './blocks1/block_128.txt',\n",
       " './blocks1/block_129.txt',\n",
       " './blocks1/block_130.txt',\n",
       " './blocks1/block_131.txt',\n",
       " './blocks1/block_132.txt',\n",
       " './blocks1/block_133.txt',\n",
       " './blocks1/block_134.txt',\n",
       " './blocks1/block_135.txt',\n",
       " './blocks1/block_136.txt',\n",
       " './blocks1/block_137.txt',\n",
       " './blocks1/block_138.txt',\n",
       " './blocks1/block_139.txt',\n",
       " './blocks1/block_140.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spimi = SPIMI(size_per_block=10240*4,\n",
    "              path_block= './.temp1/',\n",
    "              output_folder='./blocks1/',\n",
    "              ram_limit=1024*1024*1024*4,\n",
    "              size_per_block_out= 1024*4)\n",
    "\n",
    "spimi.BSBIndexConstuction(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import pandas as pd\n",
    "import bisect\n",
    "\n",
    "class CosineSimilaritySearch:\n",
    "    def __init__(self, block_folder, data):\n",
    "        self.block_folder = block_folder\n",
    "        self.data = data\n",
    "        self.num_docs = len(data)\n",
    "        self.language_map = {\n",
    "            'es': 'spanish',\n",
    "            'en': 'english',\n",
    "            'fr': 'french',\n",
    "            'de': 'german',\n",
    "            'it': 'italian'\n",
    "        }\n",
    "\n",
    "    def preprocess(self, text, lang='en'):\n",
    "        lang = self.language_map.get(lang, 'english')\n",
    "        stop_words = stopwords.words(lang)\n",
    "        stemmer = SnowballStemmer(language=lang)\n",
    "\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z0-9_À-ÿ]', ' ', text)\n",
    "\n",
    "        words = []\n",
    "        for word in text.split():\n",
    "            if word not in stop_words:\n",
    "                words.append(stemmer.stem(word))\n",
    "\n",
    "        return words\n",
    "\n",
    "    def calculate_query_vector(self, query_terms, df_dict):\n",
    "        tf_query = defaultdict(int)\n",
    "        for term in query_terms:\n",
    "            tf_query[term] += 1\n",
    "\n",
    "        query_vector = {}\n",
    "        for term, tf in tf_query.items():\n",
    "            idf = np.log((self.num_docs / df_dict[term])) if term in df_dict and df_dict[term] > 0 else 0\n",
    "            query_vector[term] = tf * idf\n",
    "            print(f\"Término: {term}, TF: {tf}, IDF: {idf}, TF-IDF: {query_vector[term]}\")  # Depuración\n",
    "        return query_vector\n",
    "    \n",
    "    def load_block_terms(self, query_terms):\n",
    "        term_postings = {}\n",
    "        df_dict = {}\n",
    "\n",
    "        for filename in os.listdir(self.block_folder):\n",
    "            with open(os.path.join(self.block_folder, filename), 'r') as file:\n",
    "                for line in file:\n",
    "                    term, rest = line.split(\" (DF: \")\n",
    "                    if self.binary_search(query_terms, term):\n",
    "                        df, postings = rest.split(\"): \")\n",
    "                        df = int(df)\n",
    "                        df_dict[term] = df\n",
    "                        \n",
    "                        term_postings[term] = []\n",
    "                        postings_list = postings.strip().split(\"), (\")\n",
    "                        for posting in postings_list:\n",
    "                            doc_id, tf = map(int, posting.strip(\"()\").split(\", \"))\n",
    "                            term_postings[term].append((doc_id, tf))\n",
    "        return term_postings, df_dict\n",
    "\n",
    "    def binary_search(self, sorted_list, item):\n",
    "        index = bisect.bisect_left(sorted_list, item)\n",
    "        return index < len(sorted_list) and sorted_list[index] == item\n",
    "\n",
    "    def cosine_similarity(self, query_vector, term_postings):\n",
    "        doc_scores = defaultdict(float)\n",
    "        query_norm = np.sqrt(np.sum(np.square(list(query_vector.values()))))\n",
    "\n",
    "        # Construcción de vectores de documentos\n",
    "        doc_vectors = defaultdict(lambda: defaultdict(float))\n",
    "        for term, query_weight in query_vector.items():\n",
    "            if term in term_postings:\n",
    "                for doc_id, tf in term_postings[term]:\n",
    "                    tf_weight = tf * query_weight\n",
    "                    doc_vectors[doc_id][term] += tf_weight\n",
    "                    print(f\"Doc: {doc_id}, Término: {term}, TF: {tf}, Peso: {tf_weight}\")  # Depuración\n",
    "\n",
    "        # Calcular similitud coseno\n",
    "        for doc_id, terms in doc_vectors.items():\n",
    "            dot_product = sum(query_vector[term] * terms[term] for term in terms if term in query_vector)\n",
    "            doc_norm = np.sqrt(sum((terms[term])**2 for term in terms))\n",
    "            if query_norm != 0 and doc_norm != 0:\n",
    "                doc_scores[doc_id] = dot_product / (query_norm * doc_norm)\n",
    "            else:\n",
    "                doc_scores[doc_id] = 0.0\n",
    "            print(f\"Doc: {doc_id}, Producto punto: {dot_product}, Norma consulta: {query_norm}, Norma documento: {doc_norm}, Similitud: {doc_scores[doc_id]}\")  # Depuración\n",
    "\n",
    "        print(doc_scores)\n",
    "\n",
    "        return sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    def get_top_k_similar_documents(self, query, lang='en', k=5):\n",
    "        query_terms = self.preprocess(query, lang=lang)\n",
    "        print(f\"Términos de la consulta procesados: {query_terms}\")\n",
    "        term_postings, df_dict = self.load_block_terms(query_terms)\n",
    "        query_vector = self.calculate_query_vector(query_terms, df_dict)\n",
    "        \n",
    "        doc_scores = self.cosine_similarity(query_vector, term_postings)\n",
    "        doc_scores = sorted(doc_scores, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "        results = []\n",
    "        for doc_id, score in doc_scores:\n",
    "            try:\n",
    "                doc_details = self.data.iloc[doc_id].to_dict()\n",
    "                doc_details[\"Cosine Similarity Score\"] = score\n",
    "                results.append(doc_details)\n",
    "            except IndexError:\n",
    "                print(f\"El documento con ID {doc_id} no está disponible en los datos.\")\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df = results_df[[\"track_name\", \"track_artist\", \"lyrics\", \"Cosine Similarity Score\"]]\n",
    "        results_df.columns = [\"Song Title\", \"Artist\", \"Lyrics\", \"Similarity Score\"]\n",
    "        results_df.index = range(1, len(results) + 1)\n",
    "        return results_df\n",
    "\n",
    "    def search(self, query, lang='en'):\n",
    "        query_terms = self.preprocess(query, lang=lang)\n",
    "        term_postings, df_dict = self.load_block_terms(query_terms)\n",
    "        query_vector = self.calculate_query_vector(query_terms, df_dict)\n",
    "        return self.cosine_similarity(query_vector, term_postings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Términos de la consulta procesados: ['mayor']\n",
      "Término: mayor, TF: 1, IDF: 5.809142990314028, TF-IDF: 5.809142990314028\n",
      "Doc: 131, Término: mayor, TF: 3, Peso: 17.42742897094208\n",
      "Doc: 476, Término: mayor, TF: 5, Peso: 29.04571495157014\n",
      "Doc: 699, Término: mayor, TF: 1, Peso: 5.809142990314028\n",
      "Doc: 131, Producto punto: 101.2384268457438, Norma consulta: 5.809142990314028, Norma documento: 17.42742897094208, Similitud: 1.0\n",
      "Doc: 476, Producto punto: 168.73071140957302, Norma consulta: 5.809142990314028, Norma documento: 29.04571495157014, Similitud: 1.0\n",
      "Doc: 699, Producto punto: 33.7461422819146, Norma consulta: 5.809142990314028, Norma documento: 5.809142990314028, Similitud: 1.0\n",
      "defaultdict(<class 'float'>, {131: np.float64(1.0), 476: np.float64(1.0), 699: np.float64(1.0)})\n",
      "Top K documentos más similares:\n",
      "       Song Title         Artist  \\\n",
      "1     Die To Live        Volbeat   \n",
      "2  Mayor Que Yo 3     Luny Tunes   \n",
      "3       Civil War  Guns N' Roses   \n",
      "\n",
      "                                              Lyrics  Similarity Score  \n",
      "1  It's 7:02, breaking all the rules Dance the bo...               1.0  \n",
      "2  Letra de \"Mayor Que Yo 3\" ft. Don Omar, Wisin,...               1.0  \n",
      "3  What we've got here is failure to communicate ...               1.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "block_folder = './blocks1/'\n",
    "search_engine = CosineSimilaritySearch(block_folder, data)\n",
    "results_df = search_engine.get_top_k_similar_documents(\"mayor que yo\", lang='es', k=5)\n",
    "print(\"Top K documentos más similares:\")\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
