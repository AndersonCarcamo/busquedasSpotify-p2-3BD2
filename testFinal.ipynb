{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ambar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from spimi import SPIMI\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testeo\n",
    "path = './dataset/'\n",
    "data_path = path + 'spotify_songs.csv'\n",
    "\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1000 = data.head(1000)\n",
    "data5000 = data.head(5000)\n",
    "data10000 = data.head(10000)\n",
    "data18000 = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 32\u001b[0m\n\u001b[1;32m     24\u001b[0m spimi\u001b[38;5;241m.\u001b[39mBSBIndexConstuction(data10000)\n\u001b[1;32m     26\u001b[0m spimi \u001b[38;5;241m=\u001b[39m SPIMI(size_per_block\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10240\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     27\u001b[0m               path_block\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./.temp18000/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     28\u001b[0m               output_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./blocks18000/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     29\u001b[0m               ram_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     30\u001b[0m               size_per_block_out\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mspimi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBSBIndexConstuction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata18000\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Archivos/bd2/proyecto2/busquedasSpotify-p2-3BD2/spimi.py:313\u001b[0m, in \u001b[0;36mBSBIndexConstuction\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc_id, row  \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m    312\u001b[0m     token_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_docs(row)\n\u001b[0;32m--> 313\u001b[0m     num_block \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspimi_invert(token_stream, doc_id, num_block)\n\u001b[1;32m    314\u001b[0m     f[num_block] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_block\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mblock_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_block\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# en caso de que el ultimo bloque no se haya escrito\u001b[39;00m\n",
      "File \u001b[0;32m~/Archivos/bd2/proyecto2/busquedasSpotify-p2-3BD2/spimi.py:68\u001b[0m, in \u001b[0;36mSPIMI.parse_docs\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m     65\u001b[0m ln \u001b[38;5;241m=\u001b[39m language_map\u001b[38;5;241m.\u001b[39mget(doc\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m texto\u001b[38;5;241m.\u001b[39msplit():\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mln\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     69\u001b[0m         words\u001b[38;5;241m.\u001b[39mappend(word)\n\u001b[1;32m     71\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m SnowballStemmer(language\u001b[38;5;241m=\u001b[39mln)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/nltk/corpus/reader/wordlist.py:22\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     20\u001b[0m         line\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m line_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw(fileids))\n\u001b[0;32m---> 22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_lines_startswith\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "spimi = SPIMI(size_per_block=10240*4,\n",
    "              path_block= './.temp1000/',\n",
    "              output_folder='./blocks1000/',\n",
    "              ram_limit=1024*1024*1024*4,\n",
    "              size_per_block_out= 1024*4)\n",
    "\n",
    "spimi.BSBIndexConstuction(data1000)\n",
    "\n",
    "\n",
    "spimi = SPIMI(size_per_block=10240*4,\n",
    "              path_block= './.temp5000/',\n",
    "              output_folder='./blocks5000/',\n",
    "              ram_limit=1024*1024*1024*4,\n",
    "              size_per_block_out= 1024*4)\n",
    "\n",
    "spimi.BSBIndexConstuction(data5000)\n",
    "\n",
    "spimi = SPIMI(size_per_block=10240*4,\n",
    "              path_block= './.temp10000/',\n",
    "              output_folder='./blocks10000/',\n",
    "              ram_limit=1024*1024*1024*4,\n",
    "              size_per_block_out= 1024*4)\n",
    "\n",
    "spimi.BSBIndexConstuction(data10000)\n",
    "\n",
    "spimi = SPIMI(size_per_block=10240*4,\n",
    "              path_block= './.temp18000/',\n",
    "              output_folder='./blocks18000/',\n",
    "              ram_limit=1024*1024*1024*4,\n",
    "              size_per_block_out= 1024*4)\n",
    "\n",
    "spimi.BSBIndexConstuction(data18000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import pandas as pd\n",
    "import bisect\n",
    "\n",
    "class CosineSimilaritySearch:\n",
    "    def __init__(self, block_folder, data):\n",
    "        self.block_folder = block_folder\n",
    "        self.data = data\n",
    "        self.num_docs = len(data)\n",
    "        self.language_map = {\n",
    "            'es': 'spanish',\n",
    "            'en': 'english',\n",
    "            'fr': 'french',\n",
    "            'de': 'german',\n",
    "            'it': 'italian'\n",
    "        }\n",
    "\n",
    "    def preprocess(self, text, lang='en'):\n",
    "        lang = self.language_map.get(lang, 'english')\n",
    "        stop_words = stopwords.words(lang)\n",
    "        stemmer = SnowballStemmer(language=lang)\n",
    "\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z0-9_À-ÿ]', ' ', text)\n",
    "\n",
    "        words = []\n",
    "        for word in text.split():\n",
    "            if word not in stop_words:\n",
    "                words.append(stemmer.stem(word))\n",
    "\n",
    "        return words\n",
    "\n",
    "    def calculate_query_vector(self, query_terms, df_dict):\n",
    "        tf_query = defaultdict(int)\n",
    "        for term in query_terms:\n",
    "            tf_query[term] += 1\n",
    "\n",
    "        query_vector = {}\n",
    "        for term, tf in tf_query.items():\n",
    "            idf = np.log((self.num_docs / df_dict[term])) if term in df_dict and df_dict[term] > 0 else 0\n",
    "            query_vector[term] = tf * idf\n",
    "            # print(f\"Término: {term}, TF: {tf}, IDF: {idf}, TF-IDF: {query_vector[term]}\")\n",
    "        return query_vector\n",
    "    \n",
    "    def load_block_terms(self, query_terms):\n",
    "        term_postings = {}\n",
    "        df_dict = {}\n",
    "\n",
    "        for filename in os.listdir(self.block_folder):\n",
    "            with open(os.path.join(self.block_folder, filename), 'r') as file:\n",
    "                for line in file:\n",
    "                    term, rest = line.split(\" (DF: \")\n",
    "                    if self.binary_search(query_terms, term):\n",
    "                        df, postings = rest.split(\"): \")\n",
    "                        df = int(df)\n",
    "                        df_dict[term] = df\n",
    "                        \n",
    "                        term_postings[term] = []\n",
    "                        postings_list = postings.strip().split(\"), (\")\n",
    "                        for posting in postings_list:\n",
    "                            doc_id, tf = map(int, posting.strip(\"()\").split(\", \"))\n",
    "                            term_postings[term].append((doc_id, tf))\n",
    "        return term_postings, df_dict\n",
    "\n",
    "    def binary_search(self, sorted_list, item):\n",
    "        index = bisect.bisect_left(sorted_list, item)\n",
    "        return index < len(sorted_list) and sorted_list[index] == item\n",
    "\n",
    "    def cosine_similarity(self, query_vector, term_postings):\n",
    "        doc_scores = defaultdict(float)  # Producto punto acumulado para cada documento\n",
    "        doc_norms = defaultdict(float)  # Acumular las normas de cada documento\n",
    "        query_norm = np.sqrt(np.sum(np.square(list(query_vector.values()))))  # Norma de la consulta\n",
    "\n",
    "        # Construcción de vectores de documentos y cálculo del producto punto\n",
    "        for term, query_weight in query_vector.items():\n",
    "            if term in term_postings:\n",
    "                for doc_id, tf in term_postings[term]:\n",
    "                    tf_weight = tf  # TF del término en el documento\n",
    "                    doc_scores[doc_id] += query_weight * tf_weight  # Producto punto acumulado\n",
    "                    doc_norms[doc_id] += tf_weight**2  # Sumar al cuadrado para calcular norma del documento\n",
    "\n",
    "        # Cálculo de similitud coseno\n",
    "        for doc_id in doc_scores:\n",
    "            doc_norm = np.sqrt(doc_norms[doc_id])  # Norma del documento\n",
    "            if query_norm != 0 and doc_norm != 0:\n",
    "                doc_scores[doc_id] = doc_scores[doc_id] / (query_norm * doc_norm)  # Similitud coseno\n",
    "            else:\n",
    "                doc_scores[doc_id] = 0.0  # Evitar división por cero\n",
    "\n",
    "        # Ordenar por similitud coseno descendente\n",
    "        return sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "    def get_top_k_similar_documents(self, query, lang='en', k=5):\n",
    "        query_terms = self.preprocess(query, lang=lang)\n",
    "        print(f\"Términos de la consulta procesados: {query_terms}\")\n",
    "        term_postings, df_dict = self.load_block_terms(query_terms)\n",
    "        query_vector = self.calculate_query_vector(query_terms, df_dict)\n",
    "        \n",
    "        doc_scores = self.cosine_similarity(query_vector, term_postings)\n",
    "        doc_scores = sorted(doc_scores, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "        results = []\n",
    "        for doc_id, score in doc_scores:\n",
    "            try:\n",
    "                doc_details = self.data.iloc[doc_id].to_dict()\n",
    "                doc_details[\"Cosine Similarity Score\"] = score\n",
    "                results.append(doc_details)\n",
    "            except IndexError:\n",
    "                print(f\"El documento con ID {doc_id} no está disponible en los datos.\")\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df = results_df[[\"track_name\", \"track_artist\", \"lyrics\", \"Cosine Similarity Score\"]]\n",
    "        results_df.columns = [\"Song Title\", \"Artist\", \"Lyrics\", \"Similarity Score\"]\n",
    "        results_df.index = range(1, len(results) + 1)\n",
    "        return results_df\n",
    "\n",
    "    def search(self, query, lang='en'):\n",
    "        query_terms = self.preprocess(query, lang=lang)\n",
    "        term_postings, df_dict = self.load_block_terms(query_terms)\n",
    "        query_vector = self.calculate_query_vector(query_terms, df_dict)\n",
    "        return self.cosine_similarity(query_vector, term_postings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Términos de la consulta procesados: ['mayor']\n",
      "Top K documentos más similares:\n",
      "       Song Title         Artist  \\\n",
      "1     Die To Live        Volbeat   \n",
      "2  Mayor Que Yo 3     Luny Tunes   \n",
      "3       Civil War  Guns N' Roses   \n",
      "\n",
      "                                              Lyrics  Similarity Score  \n",
      "1  It's 7:02, breaking all the rules Dance the bo...               1.0  \n",
      "2  Letra de \"Mayor Que Yo 3\" ft. Don Omar, Wisin,...               1.0  \n",
      "3  What we've got here is failure to communicate ...               1.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "block_folder = './blocks1/'\n",
    "search_engine = CosineSimilaritySearch(block_folder, data)\n",
    "results_df = search_engine.get_top_k_similar_documents(\"mayor que yo\", lang='es', k=5)\n",
    "print(\"Top K documentos más similares:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Términos de la consulta procesados: ['mayor']\n",
      "Top K documentos más similares:\n",
      "                                           Song Title                Artist  \\\n",
      "1                                         Die To Live               Volbeat   \n",
      "2                                      Mayor Que Yo 3            Luny Tunes   \n",
      "3                                           Civil War         Guns N' Roses   \n",
      "4                                 Rebelde por Defecto              ToteKing   \n",
      "5                                          Quiero Mas                 Ozuna   \n",
      "6                                                  Ho              Ludacris   \n",
      "7                                       Ghetto Cowboy              Mo Thugs   \n",
      "8                                              Diavla             Chris Viz   \n",
      "9                                          King Kunta        Kendrick Lamar   \n",
      "10                                            Ferrari                  Duki   \n",
      "11                                  Boogie Down Bronx           Man Parrish   \n",
      "12                                            Hubiera       Crudo Means Raw   \n",
      "13                                       Como Le Digo                  Khea   \n",
      "14                    Lifestyles of the Rich & Famous        Good Charlotte   \n",
      "15                        Arte (feat. Pablo Quintero)  Carlos Herrera Music   \n",
      "16                                  Maldita Abusadora          Paulo Londra   \n",
      "17                                              Juice            London Jae   \n",
      "18  Cross the Path (feat. Swizz Beatz, A.CHAL & Ji...   Godfather of Harlem   \n",
      "19                                   Chica Paranormal          Paulo Londra   \n",
      "20                                             Callao        Wisin & Yandel   \n",
      "21                                     Cuando Te Besé               Becky G   \n",
      "22                                      Querido Amigo          Paulo Londra   \n",
      "23                                           Eternity              Master P   \n",
      "24                                             Callao        Wisin & Yandel   \n",
      "25                                        Pa' Brillar             Lucho SSJ   \n",
      "26                             Deseos de usar y tirar               Bunbury   \n",
      "27                                    Romeo y Julieta          Paulo Londra   \n",
      "28                                              Mujer               MC Davo   \n",
      "29                                     Can I Kick It?  A Tribe Called Quest   \n",
      "30                                            Tal Vez          Paulo Londra   \n",
      "31                                     Mayor Que Yo 3            Luny Tunes   \n",
      "32                                     No te enamores        Efecto Pasillo   \n",
      "\n",
      "                                               Lyrics  Similarity Score  \n",
      "1   It's 7:02, breaking all the rules Dance the bo...          0.174076  \n",
      "2   Letra de \"Mayor Que Yo 3\" ft. Don Omar, Wisin,...          0.174076  \n",
      "3   What we've got here is failure to communicate ...          0.174076  \n",
      "4   NA Mi cuerpo un recipiente que sin música no s...          0.174076  \n",
      "5   NA Baby (oh-oh) Esa mirada era la señal que yo...          0.174076  \n",
      "6   Ho (Ho!), you's a ho (Ho!) You's a ho, I said ...          0.174076  \n",
      "7   You betta count your money You betta count you...          0.174076  \n",
      "8   NA Tu novio es un bobo No se merece ninguna mu...          0.174076  \n",
      "9   NA I got a bone to pick I don't want you monke...          0.174076  \n",
      "10  NA (Hmm-mmm, yeh, hmm-mmm) Cuando yo era un mo...          0.174076  \n",
      "11  Cool Johnski from the Freeze Force Crew I came...          0.174076  \n",
      "12  Y qué ¿más chimba allá o mas chimba acá? Aquí ...          0.174076  \n",
      "13  NA Yeah, Khea \"Young Flex\" Omar Varela Dímelo,...          0.174076  \n",
      "14  Always see it on T.V., or read in the magazine...          0.174076  \n",
      "15  Eh Aah Aah Ah Eh Aah Aah Ah Hey No quiero volv...          0.174076  \n",
      "16  NA Sé cómo eres, eso me encanta Sé lo que hici...          0.174076  \n",
      "17  NA I got 99 problems, nigga 99 flavors Big cri...          0.174076  \n",
      "18  (Ayy, oh, brrraaa) Yeah, yeah, yeah (J) Yeah, ...          0.174076  \n",
      "19  NA O-O-Ovy On The Drums Tengo mil problema' qu...          0.174076  \n",
      "20  NA Los Legendarios Señoritas, son las ligas ma...          0.174076  \n",
      "21  NA Y cuando te vi supe que no era' para mí (Pa...          0.174076  \n",
      "22  NA ¿Cómo has estado? Tú sabe' que ere' mi herm...          0.174076  \n",
      "23  This is an invitation for all my thug niggas t...          0.174076  \n",
      "24  NA Los Legendarios Señoritas, son las ligas ma...          0.174076  \n",
      "25  NA Esta chain me luce fresca pa' brillar (Pa' ...          0.174076  \n",
      "26  NA Tus ojos, espadas dentro de mi carne Me lle...          0.174076  \n",
      "27  NA Ey, ¡wuh!O-O-Ovy On The Drums Ey, yo', oh L...          0.174076  \n",
      "28  NA Woh-oh-oh-oh, woh-oh Woh-oh-oh-oh, woh-oh W...          0.174076  \n",
      "29  Can I kick it? (Yes, you can!) Can I kick it? ...          0.174076  \n",
      "30  NA Ey, oh O-O-Ovy On The Drums Ey Qué será Qué...          0.174076  \n",
      "31  Letra de \"Mayor Que Yo 3\" ft. Don Omar, Wisin,...          0.174076  \n",
      "32  NA ¿Cómo te pido que no te enamores? Eh-eh Si ...          0.174076  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "block_folder = './blocks10000/'\n",
    "search_engine = CosineSimilaritySearch(block_folder, data10000)\n",
    "results_df = search_engine.get_top_k_similar_documents(\"mayor que yo\", lang='es', k=50)\n",
    "print(\"Top K documentos más similares:\")\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
