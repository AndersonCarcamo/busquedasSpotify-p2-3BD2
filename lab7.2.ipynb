{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af630943",
   "metadata": {},
   "source": [
    "# Laborato 7.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c4d43",
   "metadata": {},
   "source": [
    "### Organización de archivos para el correcto funcionamiento\n",
    "```bash\n",
    "docs/\n",
    "    libro1.txt\n",
    "    libro2.txt\n",
    "    libro3.txt\n",
    "    libro4.txt\n",
    "    libro5.txt\n",
    "    libro6.txt\n",
    "df_total.csv\n",
    "index_file.pkl\n",
    "lab7.2.pdf\n",
    "stoplist.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e4b43",
   "metadata": {},
   "source": [
    "## P3- Matriz de similitudes\n",
    "> Elabore una matriz de similitud de coseno entre los documentos de la colección \"El Señor de los Anillos\". Debe aplicar los pesos TF-IDF. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfeda2a",
   "metadata": {},
   "source": [
    "### 1- Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "683718b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/flowers/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff15a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stoplist.txt\",encoding='latin-1') as file:\n",
    "    stoplist = [line.strip() for line in file]\n",
    "stoplist += [',','.','?','-',':',';','«','»','º','(',')']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1443e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "def preprocesamiento(texto):\n",
    "    words = []\n",
    "    # tokenizar\n",
    "    tokens = nltk.word_tokenize(texto.strip().lower())\n",
    "    # filtrar stopwords\n",
    "    for token in tokens:\n",
    "        if token not in stoplist:\n",
    "            words.append(token)\n",
    "    # reducir palabras\n",
    "    output = []\n",
    "    for word in words:\n",
    "        output.append(stemmer.stem(word))\n",
    "    #output = list(set(output))\n",
    "    return output\n",
    "\n",
    "#preprocesamiento(\"docs/libro1.txt\")\n",
    "textos = [\"libro1.txt\",\"libro2.txt\",\"libro3.txt\",\"libro4.txt\",\"libro5.txt\",\"libro6.txt\"]\n",
    "textos_procesados = []\n",
    "indice = {}\n",
    "for file_name in textos:\n",
    "  file = open('docs/'+file_name)\n",
    "  texto = file.read().rstrip()\n",
    "  texto = preprocesamiento(texto)  \n",
    "  textos_procesados.append(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d373d",
   "metadata": {},
   "source": [
    "### 2- Similitud de coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b42d081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999999999999999, 0.05669275175863886, 0.04509387616430283, 0.04806343299920239, 0.0395922331872777, 0.0918719664473555]\n",
      "[0.05669275175863886, 0.9999999999999991, 0.07179730207590511, 0.06183254395551965, 0.048636233918740786, 0.09700874413368801]\n",
      "[0.04509387616430283, 0.07179730207590511, 1.0, 0.04414591567182407, 0.06708006727456846, 0.06932511845081603]\n",
      "[0.04806343299920239, 0.06183254395551965, 0.04414591567182407, 0.9999999999999997, 0.06519740470538389, 0.08873197815455597]\n",
      "[0.0395922331872777, 0.048636233918740786, 0.06708006727456846, 0.06519740470538389, 0.9999999999999996, 0.06443242375248782]\n",
      "[0.0918719664473555, 0.09700874413368801, 0.06932511845081603, 0.08873197815455597, 0.06443242375248782, 1.0000000000000002]\n"
     ]
    }
   ],
   "source": [
    "def compute_tfidf(collection):\n",
    "  # calcular los pesos TF_IDF para cada documento de la coleccion\n",
    "\n",
    "    N = len(collection)\n",
    "    tf = [] # lista de diccionarios donde cada diccionario es un documento.\n",
    "            #key: terminos y values: frecuencias de esos terminos en el documento.\n",
    "    df = {} # Un solo diccionario donde keys: terminos \n",
    "            #y values: frecuencias en los que aparece en el documento.\n",
    "    # calculando tf\n",
    "    for doc in collection:\n",
    "        term_freq = {}\n",
    "        for term in doc:\n",
    "            if term in term_freq:\n",
    "                term_freq[term] += 1\n",
    "            else:\n",
    "                term_freq[term] = 1\n",
    "        tf.append(term_freq)\n",
    "        # calculando df\n",
    "        for term in term_freq.keys():\n",
    "            if term in df:\n",
    "                df[term] += 1\n",
    "            else:\n",
    "                df[term] = 1\n",
    "\n",
    "    # calculando tf-idf\n",
    "    tf_idf = [] # Lista de diccionarios donde cada diccionario representa un documento.\n",
    "                # key: terminos y value: peso TF-IDF de ese término en el documento.\n",
    "    for list_dic in tf:\n",
    "        dict_tfidf = {}\n",
    "        for termino,frecuencia in list_dic.items():\n",
    "            tf_value = math.log10(1 + frecuencia)\n",
    "            idf_value = math.log10(N/df[termino])\n",
    "            dict_tfidf[termino] = tf_value*idf_value\n",
    "        tf_idf.append(dict_tfidf)\n",
    "    return tf_idf\n",
    "\n",
    "def cosine_sim(Q, Doc):\n",
    "    # aplicar la similitud de coseno y construir la matriz\n",
    "    # Si Q y Doc fueran vectores numericos\n",
    "    #return np.dot(Q,doc) / (np.linalg.norm(Q) * np.linalg.norm(doc)) \n",
    "    # donde:\n",
    "    # np.dot(Q,doc) producto punto entre los vectores Q y Doc.\n",
    "    # np.linalg.norm la norma Euclidiana (longitud) del vector Q y doc. (pitagorazo)\n",
    "    \n",
    "    # Q,Doc ahora que son diccionarios \n",
    "    unique_terms = set(Q.keys()).union(set(Doc.keys()))\n",
    "\n",
    "    # np.dot(Q,doc)\n",
    "    dot_product = sum(Q.get(term, 0.0) * Doc.get(term, 0.0) for term in unique_terms)\n",
    "\n",
    "    # np.linalg.norm(Q)\n",
    "    norm_Q = math.sqrt(sum(value ** 2 for value in Q.values()))\n",
    "\n",
    "    # np.linalg.norm(Doc)\n",
    "    norm_Doc = math.sqrt(sum(value ** 2 for value in Doc.values()))\n",
    "\n",
    "    # casos nulos\n",
    "    if norm_Q == 0 or norm_Doc == 0:\n",
    "        return 0.0\n",
    "    return dot_product / (norm_Q * norm_Doc)\n",
    "\n",
    "textos_tfidf = compute_tfidf(textos_procesados)\n",
    "matriz = []\n",
    "for doc1 in textos_tfidf:\n",
    "  row = []\n",
    "  for doc2 in textos_tfidf:  \n",
    "    row.append(cosine_sim(doc1, doc2))\n",
    "  matriz.append(row)\n",
    "\n",
    "for row in matriz:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475ff4ac",
   "metadata": {},
   "source": [
    "## P4- Indice invertido con similitud de coseno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7306f700",
   "metadata": {},
   "source": [
    "### 1- Estructura del índice invertido en Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d35a0b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nindex = {\\nw1 : [(doc1, tf_w1_doc1), (doc3, tf_w1_doc3),(doc4, tf_w1_doc4),(doc10, tf_w1_doc10)],\\nw2 : [(doc1, tf_w2_doc1 ), (doc2, tf_w2_doc2)],\\nw3 : [(doc2, tf_w3_doc2), (doc3, tf_w3_doc3),(doc7, tf_w3_doc7)],\\n}\\n\\nidf = {\\nw1 : idf_w1,\\nw2 : idf_w2,\\nw3 : idf_w3,\\n}\\n\\nlength ={\\ndoc1: norm_doc1,\\ndoc2: norm_doc2,\\ndoc3: norm_doc3,\\n...\\n}\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "index = {\n",
    "w1 : [(doc1, tf_w1_doc1), (doc3, tf_w1_doc3),(doc4, tf_w1_doc4),(doc10, tf_w1_doc10)],\n",
    "w2 : [(doc1, tf_w2_doc1 ), (doc2, tf_w2_doc2)],\n",
    "w3 : [(doc2, tf_w3_doc2), (doc3, tf_w3_doc3),(doc7, tf_w3_doc7)],\n",
    "}\n",
    "\n",
    "idf = {\n",
    "w1 : idf_w1,\n",
    "w2 : idf_w2,\n",
    "w3 : idf_w3,\n",
    "}\n",
    "\n",
    "length ={\n",
    "doc1: norm_doc1,\n",
    "doc2: norm_doc2,\n",
    "doc3: norm_doc3,\n",
    "...\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27afa66",
   "metadata": {},
   "source": [
    "### 2- Algoritmo para construir el índice\n",
    "#### Paso 1:\tImplementar los métodos de la clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb2a3644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/flowers/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "with open(\"stoplist.txt\",encoding='latin-1') as file:\n",
    "    stoplist = [line.strip() for line in file]\n",
    "stoplist += [',','.','?','-',':',';','«','»','º','(',')']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4592a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertIndex: \n",
    "\n",
    "    def __init__(self, index_file):\n",
    "        self.index_file = index_file\n",
    "        self.index = defaultdict(list)\n",
    "        self.idf = {}\n",
    "        self.length = {}\n",
    "\n",
    "    def preprocesamiento(self,texto):\n",
    "        words = []\n",
    "        # tokenizar\n",
    "        tokens = nltk.word_tokenize(texto.strip().lower())\n",
    "        # filtrar stopwords\n",
    "        for token in tokens:\n",
    "            if token not in stoplist:\n",
    "                words.append(token)\n",
    "        # reducir palabras\n",
    "        output = []\n",
    "        for word in words:\n",
    "            output.append(stemmer.stem(word))\n",
    "        #output = list(set(output))\n",
    "        return output\n",
    "    \n",
    "    def load_index(self,index_file):\n",
    "        with open(index_file,'rb') as file: \n",
    "            self.index,self.idf,self.length = pickle.load(file) # que es pickle? Sirve para serializar\n",
    "    \n",
    "    def save_index(self):\n",
    "        with open(self.index_file,'wb') as file:\n",
    "            pickle.dump((self.index,self.idf,self.length),file)\n",
    "    \n",
    "    def computeTF_IDF(self,processText):\n",
    "        N = len(processText)\n",
    "        tf = []\n",
    "        df = {}\n",
    "        # calculate tf\n",
    "        for doc in processText:\n",
    "            term_freq = {}\n",
    "            for term in doc:\n",
    "                if term in term_freq:\n",
    "                    term_freq[term] += 1\n",
    "                else:\n",
    "                    term_freq[term] = 1\n",
    "            tf.append(term_freq)\n",
    "            # calculate df\n",
    "            for term in term_freq.keys():\n",
    "                if term in df:\n",
    "                    df[term] += 1\n",
    "                else:\n",
    "                    df[term] = 1\n",
    "        return tf,df\n",
    "    \n",
    "    def building(self, collection_text):\n",
    "        # build the inverted index with the collection\n",
    "        textos_procesados = []\n",
    "        for row in collection_text: # imaginar que cada fila es un documento como en el ejercicio anterior\n",
    "            textos_procesados.append(self.preprocesamiento(row))\n",
    "        # compute the tf and df\n",
    "        tf,df = self.computeTF_IDF(textos_procesados)\n",
    "        \n",
    "        # compute the idf\n",
    "        N = len(textos_procesados)\n",
    "        for term,freq in df.items():\n",
    "            self.idf[term] = math.log10(N/freq) # dictionary idf \n",
    "                \n",
    "        # compute the length (norm)\n",
    "        for doc_id, term_freq in enumerate(tf):\n",
    "            norm = 0\n",
    "            for term, freq in term_freq.items():\n",
    "                tf_idf =  math.log10(1+freq)*self.idf[term]\n",
    "                norm += tf_idf**2\n",
    "                self.index[term].append((doc_id,tf_idf)) # dictionary index\n",
    "            self.length[doc_id] = math.sqrt(norm) # dictionary length\n",
    "            \n",
    "        # store in disk\n",
    "        self.save_index()\n",
    "    \n",
    "    def getTerms(self, query):\n",
    "        # extraer los terminos unicos\n",
    "        return self.preprocesamiento(query)\n",
    "\n",
    "    def getTf_idf(self,query):\n",
    "        # calcular el tf-idf de la query\n",
    "        tf_idf_query = {}\n",
    "        # tf\n",
    "        term_freq = defaultdict(int)\n",
    "        for term in query:\n",
    "            term_freq[term] += 1\n",
    "        # tf-idf\n",
    "        for term, freq in term_freq.items():\n",
    "            if term in self.idf:\n",
    "                tf_idf_query[term] = math.log10(1+freq)*self.idf[term]\n",
    "            else:\n",
    "                tf_idf_query[term] = 0\n",
    "        return tf_idf_query\n",
    "\n",
    "    def retrieval(self, query, k): # busqueda de los top k documentos\n",
    "        self.load_index(self.index_file) # Leemos desde el disco a la RAM\n",
    "        # diccionario para el score\n",
    "        score = defaultdict(float) #{}\n",
    "        # preprocesar la query: extraer los terminos unicos\n",
    "        queryTerms = self.getTerms(query)\n",
    "        # calcular el tf-idf del query\n",
    "        tf_idf_query = self.getTf_idf(queryTerms)\n",
    "        # aplicar similitud de coseno y guardarlo en el diccionario score\n",
    "        query_norm = math.sqrt(sum(value**2 for value in tf_idf_query.values()))\n",
    "        \n",
    "        for term,weight in tf_idf_query.items():\n",
    "            if term in self.index:\n",
    "                for doc_id, tf_idf in self.index[term]:\n",
    "                    score[doc_id] += tf_idf * weight\n",
    "        for doc_id in score:\n",
    "            score[doc_id] /= (self.length[doc_id]*query_norm)\n",
    "        # ordenar el score de forma descendente\n",
    "        result = sorted(score.items(), key= lambda tup: tup[1], reverse=True)\n",
    "        # retornamos los k documentos mas relevantes (de mayor similitud al query)\n",
    "        return result[:k]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c535840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataton = pd.read_csv('df_total.csv')\n",
    "index_file = 'index_file.pkl'\n",
    "invert_index = InvertIndex(index_file)\n",
    "collection_text = list(dataton['news'])\n",
    "invert_index.building(collection_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ed2a2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"La importancia del desarrollo en la sostenibilidad\",\n",
    "    \"El pais de China y su cooperacion\",\n",
    "    \"Economista importante de México\",\n",
    "    \"Perú, delincuencia\"\n",
    "]\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "615f27da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "Results for Query 1: La importancia del desarrollo en la sostenibilidad\n",
      "Document ID: 0, Score: 0.16247797364379105\n",
      "Document ID: 917, Score: 0.14599615352698717\n",
      "Document ID: 1076, Score: 0.13676608530187134\n",
      "Document ID: 725, Score: 0.12402367954345637\n",
      "Document ID: 601, Score: 0.11339766409182\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Results for Query 2: El pais de China y su cooperacion\n",
      "Document ID: 1, Score: 0.20882922202200407\n",
      "Document ID: 15, Score: 0.14379238701980368\n",
      "Document ID: 34, Score: 0.12640467194074168\n",
      "Document ID: 157, Score: 0.11341349144334985\n",
      "Document ID: 441, Score: 0.11081064693756089\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Results for Query 3: Economista importante de México\n",
      "Document ID: 313, Score: 0.19927392391962723\n",
      "Document ID: 485, Score: 0.19927392391962723\n",
      "Document ID: 648, Score: 0.17831018762694886\n",
      "Document ID: 981, Score: 0.17831018762694886\n",
      "Document ID: 28, Score: 0.1461234033608242\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Results for Query 4: Perú, delincuencia\n",
      "Document ID: 754, Score: 0.18971503098931244\n",
      "Document ID: 758, Score: 0.1021420543357743\n",
      "Document ID: 1101, Score: 0.09252592903639564\n",
      "Document ID: 444, Score: 0.08735476313914237\n",
      "Document ID: 553, Score: 0.08256279837549359\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def search_and_print_results(invert_index, queries, k):\n",
    "    for i, query in enumerate(queries, start=1):\n",
    "        print(f\"------------------------------------\")\n",
    "        print(f\"Results for Query {i}: {query}\")\n",
    "        result = invert_index.retrieval(query, k)\n",
    "        for doc_id, score in result:\n",
    "            print(f\"Document ID: {doc_id}, Score: {score}\")\n",
    "        print(f\"------------------------------------\")\n",
    "search_and_print_results(invert_index, queries, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1b81656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Para julio de 2022 el Indicador de Confianza Empresarial ICE realizado por el Departamento Administrativo de Planeación Nacional de Estadísticas Dane para las empresas de comercio industria manufacturera servicios y construcción fue de 592 es decir 34 puntos porcentuales menos al que se registró en el mes anterior.Visto desde la parte de los sectores se conoció que las empresas dedicadas a servicios tuvieron una puntuación de 616 siendo la de mejor puntuación. Seguida están el sector de comercio con 59 puntos registrando junto con la anterior el valor más alto del indicador para julio de este año. A su vez el sector de industria manufacturera tuvo puntuación de 589 y construcción 507.Los resultados que más destacaron en la encuesta fue que en junio de 2022 441 de las empresas de los cuatro sectores estaban siendo gerenciadas o administradas por una mujer mientras que 559 por hombres. Por otra parte de estos cuatro sectores 975 de las empresas reportaron una operación normal para junio de 2022 y las empresas de comercio registraron el porcentaje más alto con 983.También para junio 337 del total de unidades de estos sectores utilizó internet para la venta de productos 34 para la venta de insumos y 456 para medios de pago. Teniendo en cuenta este indicador para julio de 2022 556 de las empresas esperaba un aumento en sus ingresos durante los próximos tres meses y en comercio 577 de las empresas esperaba contar con este incremento.Finalmente en términos de delincuencia 579 de las empresas encuestadas pensaban que operar en Colombia es inseguro. A su vez en julio 775 de los empresarios esperaban que la delincuencia siga igual en el próximo mes 123 opina que empeorará y 102 que mejorará.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Puedes corroborar manualmente aqui, poniendo el ID = indice\n",
    "dataton = pd.read_csv('df_total.csv')\n",
    "dataton['news'][754]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
