{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ambar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# librerias\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sortedcontainers import SortedDict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import regex as re\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_per_block = 10240*4 # 4 KBytes\n",
    "num_block = 0\n",
    "path_block = './blocks3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostingBlock:\n",
    "    def __init__(self, max_size=10):\n",
    "        self.doc_dict = {}  # Diccionario para almacenar postings como {doc_id: TF}\n",
    "        self.next_block = None\n",
    "        self.max_size = max_size\n",
    "        # self.current_size = 0\n",
    "\n",
    "    def is_full(self):\n",
    "        return len(self.doc_dict) >= self.max_size\n",
    "\n",
    "    def add_doc(self, doc_id, tf=1):\n",
    "        if doc_id in self.doc_dict:\n",
    "            self.doc_dict[doc_id] += tf\n",
    "        else:\n",
    "            self.doc_dict[doc_id] = tf\n",
    "            # self.current_size += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParseDocs(doc):\n",
    "    words = []\n",
    "\n",
    "    col_text = ['lyrics', 'track_name', 'track_artist', 'track_album_name', 'playlist_name', 'playlist_genre', 'playlist_subgenre']\n",
    "    texto = ''\n",
    "    for col in col_text:\n",
    "        texto += ' ' + doc[col]\n",
    "\n",
    "    texto = texto.lower()\n",
    "\n",
    "    texto = re.sub(r'[^a-zA-Z0-9_À-ÿ]', ' ', texto)\n",
    "\n",
    "\n",
    "    if doc['language'] == 'es':\n",
    "        ln = 'spanish'\n",
    "    elif doc['language'] == 'tl':\n",
    "        ln = 'english'\n",
    "    else:\n",
    "        ln = 'english'\n",
    "    \n",
    "    for word in texto.split():\n",
    "        \n",
    "        if word not in stopwords.words(ln):\n",
    "            words.append(word)\n",
    "\n",
    "    # aplicar stemming\n",
    "    stemmer = SnowballStemmer(language=ln)\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WriteBlockToDisk(dictionary, block_id, path='./blocks2/'):\n",
    "    output_file = f\"{path}block_{block_id}.txt\"\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        for term, data in dictionary.items():\n",
    "            df = data['df']\n",
    "            postings_list = []\n",
    "\n",
    "            posting_block = data['posting_block']\n",
    "            while posting_block is not None:\n",
    "                postings_list.extend([f\"({doc_id}, {tf})\" for doc_id, tf in posting_block.doc_dict.items()])\n",
    "                posting_block = posting_block.next_block  # Avanzar al siguiente bloque enlazado\n",
    " \n",
    "            postings = ', '.join(postings_list)\n",
    "            f.write(f\"{term} (DF: {df}): {postings}\\n\")\n",
    "    \n",
    "    # print(f\"Block {block_id} written to disk as {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SPIMI_INVERT(token_stream, # recibo una lista de lexemas\n",
    "                 docId, # el numero del documento\n",
    "                 num_block, # el numero del bloque en el que escribo\n",
    "                 size_per_block,\n",
    "                 path_block = './blocks/'\n",
    "                 ):\n",
    "    \n",
    "    # name_block_archive = path_block + f'block_{num_block}.txt'\n",
    "    # outputFile = open(name_block_archive, 'w') # NewFIle()\n",
    "\n",
    "    global global_dictionary, global_block_size\n",
    "\n",
    "    # dictionary = {} # NewHash() - Diccionario invertido parcial\n",
    "    # block_size = 0\n",
    "\n",
    "    \n",
    "    for token in token_stream:\n",
    "\n",
    "        if token not in global_dictionary:\n",
    "            posting_block = PostingBlock()\n",
    "            global_dictionary[token] = {\n",
    "                'df': 1,\n",
    "                'posting_block': posting_block\n",
    "            }\n",
    "            posting_block.add_doc(docId, 1)\n",
    "            global_block_size += sys.getsizeof(token) + sys.getsizeof(global_dictionary[token])\n",
    "            \n",
    "        else:\n",
    "            # posting_list = GetPostingsList(dictionary, term(token))\n",
    "            posting_block = global_dictionary[token]['posting_block']\n",
    "            doc_found = False\n",
    "\n",
    "            while posting_block is not None:\n",
    "                if docId in posting_block.doc_dict:\n",
    "                    # Incrementa TF si el doc_id ya existe\n",
    "                    posting_block.doc_dict[docId] += 1\n",
    "                    doc_found = True\n",
    "                    break\n",
    "                \n",
    "                # Avanzar al siguiente bloque si es necesario\n",
    "                if posting_block.next_block is None:\n",
    "                    break\n",
    "                else:\n",
    "                    posting_block = posting_block.next_block\n",
    "  \n",
    "            # Si el documento no fue encontrado en ningún bloque\n",
    "            if not doc_found:\n",
    "                global_dictionary[token]['df'] += 1\n",
    "                # Si el bloque actual está lleno, enlazar un nuevo bloque\n",
    "                if posting_block.is_full():\n",
    "                    new_posting_block = PostingBlock()\n",
    "                    posting_block.next_block = new_posting_block\n",
    "                    posting_block = new_posting_block\n",
    "\n",
    "                posting_block.add_doc(docId, 1)\n",
    "                global_block_size += sys.getsizeof(docId)\n",
    "\n",
    "        # Si el bloque alcanza el límite de tamaño, escribir a disco y resetear el diccionario\n",
    "        if global_block_size >= size_per_block:\n",
    "            WriteBlockToDisk(global_dictionary, num_block, path_block)\n",
    "            global_dictionary = SortedDict()\n",
    "            global_block_size = 0\n",
    "            num_block += 1\n",
    "    \n",
    "    # sorted_terms = SortTerms(dictionary)\n",
    "    # sorted_terms = SorTerms(dictionary)\n",
    "    # aca ya no es necesario el orden, pues el diccionario global ya esta ordenado\n",
    "\n",
    "    # ya no es necesario el writeBlock porque se hara con el proximo bloque recivido\n",
    "    # if global_dictionary:\n",
    "    #     WriteBlockToDisk(global_dictionary, num_block, path=path_block)\n",
    "    # WriteBlocksToDisk(sorted_terms, dictionary,outputFile)\n",
    "    return num_block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BSBIndexConstuction(data):\n",
    "\n",
    "    global global_dictionary, global_block_size\n",
    "\n",
    "    if not os.path.exists(path_block):\n",
    "        os.makedirs(path_block)\n",
    "\n",
    "    block_n = 0 # numero de bloque actual \n",
    "\n",
    "    f = {} # archivos de bloques\n",
    "    for doc_id, row  in data.iterrows():\n",
    "\n",
    "        token_stream = ParseDocs(row)\n",
    "        num_block = SPIMI_INVERT(token_stream, doc_id, num_block=block_n,  size_per_block=size_per_block, path_block=path_block,)\n",
    "        block_n = num_block\n",
    "        f[num_block] = f'{path_block}block_{num_block}.txt'\n",
    "    \n",
    "    # en caso de que el ultimo bloque no se haya escrito\n",
    "    if global_dictionary:\n",
    "        WriteBlockToDisk(global_dictionary, num_block, path=path_block)\n",
    "        f[num_block] = f'{path_block}block_{num_block}.txt'\n",
    "        num_block += 1\n",
    "        # innecesario pero me sirve para indicar que se acabo y limpiarlo\n",
    "        global_dictionary = SortedDict()\n",
    "        global_block_size = 0\n",
    "\n",
    "\n",
    "    # print(f'Numero de bloques: {num_block}')    \n",
    "\n",
    "    # f = MergeBlocks(f)\n",
    "    # shutil.rmtree(path_block)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testeo\n",
    "path = './dataset/'\n",
    "data_path = path + 'spotify_songs_test.csv'\n",
    "\n",
    "global_dictionary = SortedDict()\n",
    "global_block_size = 0\n",
    "\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bloques_merged = BSBIndexConstuction(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_block_line(line):\n",
    "    \"\"\"Parsea una línea del archivo de bloque en el formato (término, DF, PostingBlock).\"\"\"\n",
    "    # print(f\"Processing line: {line}\")\n",
    "    term, rest = line.split(\" (DF: \")\n",
    "    df, postings = rest.split(\"): \")\n",
    "    df = int(df)\n",
    "    posting_block = PostingBlock()\n",
    "\n",
    "    postings_list = postings.strip(\"()\").split(\"), (\")\n",
    "    # print(f'postings_list: {postings_list}')\n",
    "    for posting in postings_list:\n",
    "        # Extraer doc_id y tf\n",
    "        doc_id, tf = map(int, posting.strip(\"()\").split(\", \"))\n",
    "        posting_block.add_doc(doc_id, tf)\n",
    "    return term, df, posting_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_output_block(block_id, \n",
    "                          output_terms, \n",
    "                          output_folder):\n",
    "    \"\"\"Escribe los términos y postings en un bloque de salida de 4 KB.\"\"\"\n",
    "    output_file = os.path.join(output_folder, f\"block_{block_id}.txt\")\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        for term, (df, posting_block) in output_terms.items():\n",
    "            term_entry = f\"{term} (DF: {df}): \"\n",
    "            postings = []\n",
    "\n",
    "            # Recorre todos los bloques de postings enlazados\n",
    "            current_block = posting_block\n",
    "            while current_block is not None:\n",
    "                postings.extend([f\"({doc}, {tf})\" for doc, tf in current_block.doc_dict.items()])\n",
    "                current_block = current_block.next_block\n",
    "\n",
    "            # Unir todos los postings en una cadena y escribir la entrada completa\n",
    "            term_entry += ', '.join(postings) + \"\\n\"\n",
    "            f.write(term_entry)\n",
    "\n",
    "    print(f\"Block {block_id} written to {output_file}\")\n",
    "    return block_id + 1, output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MergeBlocks(block_files, \n",
    "                output_folder='./merged_blocks/', \n",
    "                block_size_limit=4096, \n",
    "                max_ram_limit=1024*1024*1024):\n",
    "    \n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Abrir archivos de bloques\n",
    "    open_files = {file_id: open(file_path, 'r') for file_id, file_path in block_files.items()}\n",
    "    \n",
    "    # inicializar buffers de entrada\n",
    "    input_buffers = {}\n",
    "    current_ram_usage = 0\n",
    "    # dedico la ram disponible quitando el del bloque de output, que seran las lineas que voy a traer de cada bloque\n",
    "    max_memory_per_block = (max_ram_limit - block_size_limit) // len(block_files) \n",
    "    merged_files = []\n",
    "    \n",
    "    heap = []\n",
    "    for file_id, file in open_files.items():\n",
    "        buffer = []\n",
    "        buffer_size = 0\n",
    "        while buffer_size < max_memory_per_block:\n",
    "            line = file.readline().strip()\n",
    "            if not line:\n",
    "                break  # Si no hay más líneas en el archivo, salir del bucle\n",
    "            term, df, posting_block = parse_block_line(line)\n",
    "            buffer.append((term, df, posting_block))\n",
    "            current_ram_usage += sys.getsizeof(term) + sys.getsizeof(df) + sys.getsizeof(posting_block)\n",
    "        \n",
    "        # Añadir el buffer al diccionario de buffers y cargar sus términos en el heap\n",
    "        input_buffers[file_id] = buffer\n",
    "        \n",
    "        for term, df, posting_block in buffer:\n",
    "            found = False\n",
    "            for i, (existing_term, existing_df, existing_file_id, existing_posting_block) in enumerate(heap):\n",
    "                if existing_term == term:\n",
    "                    existing_df += df\n",
    "                    current_block = existing_posting_block\n",
    "                    while current_block.next_block:\n",
    "                        current_block = current_block.next_block\n",
    "                    if current_block.is_full():\n",
    "                        current_block.next_block = posting_block\n",
    "                    else:\n",
    "                        for doc, tf in posting_block.doc_dict.items():\n",
    "                            if current_block.is_full():\n",
    "                                new_block = PostingBlock()\n",
    "                                current_block.next_block = new_block\n",
    "                                current_block = new_block\n",
    "                            current_block.add_doc(doc, tf)\n",
    "                    heap[i] = (existing_term, existing_df, existing_file_id, existing_posting_block)\n",
    "                    found = True\n",
    "                    break\n",
    "            \n",
    "            if not found:\n",
    "                heapq.heappush(heap, (term, df, file_id, posting_block))\n",
    "\n",
    "\n",
    "    # print(\"Initial heap content:\")\n",
    "    # for item in heap:\n",
    "    #     print(item)\n",
    "\n",
    "    block_id = 0\n",
    "    output_terms = {}\n",
    "    current_output_size = 0\n",
    "\n",
    "    while heap:\n",
    "        # Extraer el término mínimo del heap\n",
    "        term, df, file_id, posting_block = heapq.heappop(heap)\n",
    "\n",
    "        # Fusionar postings si el término ya existe en output_terms\n",
    "        if term in output_terms:\n",
    "            existing_df, existing_posting_block = output_terms[term]\n",
    "            output_terms[term] = (existing_df + df, existing_posting_block)\n",
    "            current_block = existing_posting_block\n",
    "            while current_block.next_block:\n",
    "                current_block = current_block.next_block\n",
    "            if current_block.is_full():\n",
    "                current_block.next_block = posting_block\n",
    "            else:\n",
    "                for doc, tf in posting_block.doc_dict.items():\n",
    "                    if current_block.is_full():\n",
    "                        new_block = PostingBlock()\n",
    "                        current_block.next_block = new_block\n",
    "                        current_block = new_block\n",
    "                    current_block.add_doc(doc, tf)\n",
    "            \n",
    "            # Solo sumar el tamaño de los nuevos postings agregados\n",
    "            additional_postings_size = len(', '.join([f\"({doc}, {tf})\" for doc, tf in posting_block.doc_dict.items()]))\n",
    "            current_output_size += additional_postings_size\n",
    "        else:\n",
    "            output_terms[term] = (df, posting_block)\n",
    "\n",
    "            # Calcular el tamaño en memoria del término y postings\n",
    "            # term_size = sys.getsizeof(term) + sys.getsizeof(df) + sys.agetsizeof(posting_block)\n",
    "            postings_str = ', '.join([f\"({doc}, {tf})\" for doc, tf in posting_block.doc_dict.items()])\n",
    "            term_entry = f\"{term} (DF: {df}): {postings_str}\\n\"\n",
    "            term_size = len(term_entry)\n",
    "            current_output_size += term_size\n",
    "\n",
    "        # Escribir el bloque de salida cuando se alcanza el límite\n",
    "        if current_output_size > block_size_limit:\n",
    "            # print(f'Tamaño hasta ahora: {current_output_size}')\n",
    "            block_id, block_file = write_to_output_block(block_id, output_terms, output_folder)\n",
    "            merged_files.append(block_file)\n",
    "            output_terms.clear()\n",
    "            current_output_size = 0\n",
    "        \n",
    "        # Recargar el buffer si está vacío\n",
    "        if not input_buffers[file_id]:\n",
    "            # Cargar nuevas líneas hasta llenar el buffer o alcanzar el límite de RAM\n",
    "            buffer = []\n",
    "            buffer_size = 0\n",
    "            while buffer_size < max_memory_per_block:\n",
    "                line = open_files[file_id].readline().strip()\n",
    "                if not line:\n",
    "                    break  # No hay más líneas en el archivo\n",
    "                term, df, posting_block = parse_block_line(line)\n",
    "                buffer.append((term, df, posting_block))\n",
    "                buffer_size += sys.getsizeof(term) + sys.getsizeof(df) + sys.getsizeof(posting_block)\n",
    "            \n",
    "            input_buffers[file_id] = buffer  # Actualizar el buffer\n",
    "\n",
    "        # Agregar el siguiente término al heap\n",
    "        if input_buffers[file_id]:\n",
    "            next_term, next_df, next_posting_block = input_buffers[file_id].pop(0)\n",
    "            heapq.heappush(heap, (next_term, next_df, file_id, next_posting_block))\n",
    "\n",
    "    # Escribir el último bloque de salida si hay términos restantes\n",
    "    if output_terms:\n",
    "        block_id, block_file = write_to_output_block(block_id, output_terms, output_folder)\n",
    "        merged_files.append(block_file)\n",
    "\n",
    "    for file in open_files.values():\n",
    "        file.close()\n",
    "\n",
    "    return merged_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MergeBlocks(block_files, \n",
    "                output_folder='./merged_blocks/', \n",
    "                block_size_limit=4096, \n",
    "                max_ram_limit=1024*1024*1024):\n",
    "    \n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    open_files = {file_id: open(file_path, 'r') for file_id, file_path in block_files.items()}\n",
    "    input_buffers = {}\n",
    "    current_ram_usage = 0\n",
    "    max_memory_per_block = (max_ram_limit - block_size_limit) // len(block_files)\n",
    "    merged_files = []\n",
    "    heap = []\n",
    "\n",
    "    # Cargar los buffers iniciales y añadir al heap con fusión de términos duplicados\n",
    "    for file_id, file in open_files.items():\n",
    "        buffer = []\n",
    "        buffer_size = 0\n",
    "        while buffer_size < max_memory_per_block:\n",
    "            line = file.readline().strip()\n",
    "            if not line:\n",
    "                break\n",
    "            term, df, posting_block = parse_block_line(line)\n",
    "            buffer.append((term, df, posting_block))\n",
    "            buffer_size += sys.getsizeof(term) + sys.getsizeof(df) + sys.getsizeof(posting_block)\n",
    "        \n",
    "        input_buffers[file_id] = buffer\n",
    "        \n",
    "        # Insertar elementos del buffer en el heap con fusión continua\n",
    "        for term, df, posting_block in buffer:\n",
    "            found = False\n",
    "            for i, (existing_term, existing_df, existing_file_id, existing_posting_block) in enumerate(heap):\n",
    "                if existing_term == term:\n",
    "                    existing_df += df\n",
    "                    current_block = existing_posting_block\n",
    "                    while current_block.next_block:\n",
    "                        current_block = current_block.next_block\n",
    "                    if current_block.is_full():\n",
    "                        current_block.next_block = posting_block\n",
    "                    else:\n",
    "                        for doc, tf in posting_block.doc_dict.items():\n",
    "                            if current_block.is_full():\n",
    "                                new_block = PostingBlock()\n",
    "                                current_block.next_block = new_block\n",
    "                                current_block = new_block\n",
    "                            current_block.add_doc(doc, tf)\n",
    "                    heap[i] = (existing_term, existing_df, existing_file_id, existing_posting_block)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                heapq.heappush(heap, (term, df, file_id, posting_block))\n",
    "\n",
    "    block_id = 0\n",
    "    output_terms = {}\n",
    "    current_output_size = 0\n",
    "\n",
    "    # Procesar el heap hasta vaciarlo\n",
    "    while heap:\n",
    "        term, df, file_id, posting_block = heapq.heappop(heap)\n",
    "\n",
    "        # Fusionar postings si el término ya existe en output_terms\n",
    "        if term in output_terms:\n",
    "            existing_df, existing_posting_block = output_terms[term]\n",
    "            output_terms[term] = (existing_df + df, existing_posting_block)\n",
    "            current_block = existing_posting_block\n",
    "            while current_block.next_block:\n",
    "                current_block = current_block.next_block\n",
    "            if current_block.is_full():\n",
    "                current_block.next_block = posting_block\n",
    "            else:\n",
    "                for doc, tf in posting_block.doc_dict.items():\n",
    "                    if current_block.is_full():\n",
    "                        new_block = PostingBlock()\n",
    "                        current_block.next_block = new_block\n",
    "                        current_block = new_block\n",
    "                    current_block.add_doc(doc, tf)\n",
    "            additional_postings_size = len(', '.join([f\"({doc}, {tf})\" for doc, tf in posting_block.doc_dict.items()]))\n",
    "            current_output_size += additional_postings_size\n",
    "        else:\n",
    "            output_terms[term] = (df, posting_block)\n",
    "            postings_str = ', '.join([f\"({doc}, {tf})\" for doc, tf in posting_block.doc_dict.items()])\n",
    "            term_entry = f\"{term} (DF: {df}): {postings_str}\\n\"\n",
    "            term_size = len(term_entry)\n",
    "            current_output_size += term_size\n",
    "\n",
    "        # Escribir el bloque de salida cuando se alcanza el límite\n",
    "        if current_output_size > block_size_limit:\n",
    "            block_id, block_file = write_to_output_block(block_id, output_terms, output_folder)\n",
    "            merged_files.append(block_file)\n",
    "            output_terms.clear()\n",
    "            current_output_size = 0\n",
    "\n",
    "        # Recargar el buffer si se queda vacío\n",
    "        if not input_buffers[file_id]:\n",
    "            buffer = []\n",
    "            buffer_size = 0\n",
    "            while buffer_size < max_memory_per_block:\n",
    "                line = open_files[file_id].readline().strip()\n",
    "                if not line:\n",
    "                    break\n",
    "                term, df, posting_block = parse_block_line(line)\n",
    "                buffer.append((term, df, posting_block))\n",
    "                buffer_size += sys.getsizeof(term) + sys.getsizeof(df) + sys.getsizeof(posting_block)\n",
    "            input_buffers[file_id] = buffer\n",
    "\n",
    "            # Agregar nuevos términos del buffer al heap con fusión de duplicados\n",
    "            for term, df, posting_block in buffer:\n",
    "                found = False\n",
    "                for i, (existing_term, existing_df, existing_file_id, existing_posting_block) in enumerate(heap):\n",
    "                    if existing_term == term:\n",
    "                        existing_df += df\n",
    "                        current_block = existing_posting_block\n",
    "                        while current_block.next_block:\n",
    "                            current_block = current_block.next_block\n",
    "                        if current_block.is_full():\n",
    "                            current_block.next_block = posting_block\n",
    "                        else:\n",
    "                            for doc, tf in posting_block.doc_dict.items():\n",
    "                                if current_block.is_full():\n",
    "                                    new_block = PostingBlock()\n",
    "                                    current_block.next_block = new_block\n",
    "                                    current_block = new_block\n",
    "                                current_block.add_doc(doc, tf)\n",
    "                        heap[i] = (existing_term, existing_df, existing_file_id, existing_posting_block)\n",
    "                        found = True\n",
    "                        break\n",
    "                if not found:\n",
    "                    heapq.heappush(heap, (term, df, file_id, posting_block))\n",
    "\n",
    "    # Escribir cualquier término restante en el último bloque\n",
    "    if output_terms:\n",
    "        block_id, block_file = write_to_output_block(block_id, output_terms, output_folder)\n",
    "        merged_files.append(block_file)\n",
    "\n",
    "    # Cerrar todos los archivos\n",
    "    for file in open_files.values():\n",
    "        file.close()\n",
    "\n",
    "    return merged_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: './blocks3/block_0.txt', 1: './blocks3/block_1.txt', 2: './blocks3/block_2.txt', 4: './blocks3/block_4.txt', 6: './blocks3/block_6.txt', 7: './blocks3/block_7.txt', 8: './blocks3/block_8.txt', 9: './blocks3/block_9.txt', 10: './blocks3/block_10.txt', 11: './blocks3/block_11.txt', 12: './blocks3/block_12.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(bloques_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0 written to ./merged_index/block_0.txt\n",
      "Block 1 written to ./merged_index/block_1.txt\n",
      "Block 2 written to ./merged_index/block_2.txt\n",
      "Block 3 written to ./merged_index/block_3.txt\n",
      "Block 4 written to ./merged_index/block_4.txt\n",
      "Block 5 written to ./merged_index/block_5.txt\n",
      "Block 6 written to ./merged_index/block_6.txt\n",
      "Block 7 written to ./merged_index/block_7.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./merged_index/block_0.txt',\n",
       " './merged_index/block_1.txt',\n",
       " './merged_index/block_2.txt',\n",
       " './merged_index/block_3.txt',\n",
       " './merged_index/block_4.txt',\n",
       " './merged_index/block_5.txt',\n",
       " './merged_index/block_6.txt',\n",
       " './merged_index/block_7.txt']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = './merged_index/'\n",
    "block_limit_size = 1024*5.9\n",
    "max_ram_limit = 1024*1024*1024\n",
    "\n",
    "MergeBlocks(bloques_merged, output_path, block_limit_size, max_ram_limit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
