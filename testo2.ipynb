{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ambar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# librerias\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sortedcontainers import SortedDict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import regex as re\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_per_block = 10240*4 # 4 KBytes\n",
    "num_block = 0\n",
    "path_block = './blocks3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostingBlock:\n",
    "    def __init__(self, max_size=10):\n",
    "        self.doc_dict = {}  # Diccionario para almacenar postings como {doc_id: TF}\n",
    "        self.next_block = None\n",
    "        self.max_size = max_size\n",
    "        self.current_size = 0\n",
    "\n",
    "    def is_full(self):\n",
    "        return len(self.doc_dict) >= self.max_size\n",
    "\n",
    "    def add_doc(self, doc_id, tf=1):\n",
    "        if doc_id in self.doc_dict:\n",
    "            self.doc_dict[doc_id] += tf\n",
    "        else:\n",
    "            self.doc_dict[doc_id] = tf\n",
    "            self.current_size += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParseDocs(doc):\n",
    "    words = []\n",
    "\n",
    "    col_text = ['lyrics', 'track_name', 'track_artist', 'track_album_name', 'playlist_name', 'playlist_genre', 'playlist_subgenre']\n",
    "    texto = ''\n",
    "    for col in col_text:\n",
    "        texto += ' ' + doc[col]\n",
    "\n",
    "    texto = texto.lower()\n",
    "\n",
    "    texto = re.sub(r'[^a-zA-Z0-9_À-ÿ]', ' ', texto)\n",
    "\n",
    "\n",
    "    if doc['language'] == 'es':\n",
    "        ln = 'spanish'\n",
    "    elif doc['language'] == 'tl':\n",
    "        ln = 'english'\n",
    "    else:\n",
    "        ln = 'english'\n",
    "    \n",
    "    for word in texto.split():\n",
    "        \n",
    "        if word not in stopwords.words(ln):\n",
    "            words.append(word)\n",
    "\n",
    "    # aplicar stemming\n",
    "    stemmer = SnowballStemmer(language=ln)\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WriteBlockToDisk(dictionary, block_id, path='./blocks2/'):\n",
    "    output_file = f\"{path}block_{block_id}.txt\"\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        for term, data in dictionary.items():\n",
    "            df = data['df']\n",
    "            postings_list = []\n",
    "\n",
    "            posting_block = data['posting_block']\n",
    "            while posting_block is not None:\n",
    "                postings_list.extend([f\"({doc_id}, {tf})\" for doc_id, tf in posting_block.doc_dict.items()])\n",
    "                posting_block = posting_block.next_block  # Avanzar al siguiente bloque enlazado\n",
    " \n",
    "            postings = ', '.join(postings_list)\n",
    "            f.write(f\"{term} (DF: {df}): {postings}\\n\")\n",
    "    \n",
    "    # print(f\"Block {block_id} written to disk as {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SPIMI_INVERT(token_stream, # recibo una lista de lexemas\n",
    "                 docId, # el numero del documento\n",
    "                 num_block, # el numero del bloque en el que escribo\n",
    "                 size_per_block,\n",
    "                 path_block = './blocks/'\n",
    "                 ):\n",
    "    \n",
    "    # name_block_archive = path_block + f'block_{num_block}.txt'\n",
    "    # outputFile = open(name_block_archive, 'w') # NewFIle()\n",
    "\n",
    "    global global_dictionary, global_block_size\n",
    "\n",
    "    # dictionary = {} # NewHash() - Diccionario invertido parcial\n",
    "    # block_size = 0\n",
    "\n",
    "    \n",
    "    for token in token_stream:\n",
    "\n",
    "        if token not in global_dictionary:\n",
    "            posting_block = PostingBlock()\n",
    "            global_dictionary[token] = {\n",
    "                'df': 1,\n",
    "                'posting_block': posting_block\n",
    "            }\n",
    "            posting_block.add_doc(docId, 1)\n",
    "            global_block_size += sys.getsizeof(token) + sys.getsizeof(global_dictionary[token])\n",
    "            \n",
    "        else:\n",
    "            # posting_list = GetPostingsList(dictionary, term(token))\n",
    "            posting_block = global_dictionary[token]['posting_block']\n",
    "            doc_found = False\n",
    "\n",
    "            while posting_block is not None:\n",
    "                if docId in posting_block.doc_dict:\n",
    "                    # Incrementa TF si el doc_id ya existe\n",
    "                    posting_block.doc_dict[docId] += 1\n",
    "                    doc_found = True\n",
    "                    break\n",
    "                \n",
    "                # Avanzar al siguiente bloque si es necesario\n",
    "                if posting_block.next_block is None:\n",
    "                    break\n",
    "                else:\n",
    "                    posting_block = posting_block.next_block\n",
    "  \n",
    "            # Si el documento no fue encontrado en ningún bloque\n",
    "            if not doc_found:\n",
    "                global_dictionary[token]['df'] += 1\n",
    "                # Si el bloque actual está lleno, enlazar un nuevo bloque\n",
    "                if posting_block.is_full():\n",
    "                    new_posting_block = PostingBlock()\n",
    "                    posting_block.next_block = new_posting_block\n",
    "                    posting_block = new_posting_block\n",
    "\n",
    "                posting_block.add_doc(docId, 1)\n",
    "                global_block_size += sys.getsizeof(docId)\n",
    "\n",
    "        # Si el bloque alcanza el límite de tamaño, escribir a disco y resetear el diccionario\n",
    "        if global_block_size >= size_per_block:\n",
    "            WriteBlockToDisk(global_dictionary, num_block, path_block)\n",
    "            global_dictionary = SortedDict()\n",
    "            global_block_size = 0\n",
    "            num_block += 1\n",
    "    \n",
    "    # sorted_terms = SortTerms(dictionary)\n",
    "    # sorted_terms = SorTerms(dictionary)\n",
    "    # aca ya no es necesario el orden, pues el diccionario global ya esta ordenado\n",
    "\n",
    "    # ya no es necesario el writeBlock porque se hara con el proximo bloque recivido\n",
    "    # if global_dictionary:\n",
    "    #     WriteBlockToDisk(global_dictionary, num_block, path=path_block)\n",
    "    # WriteBlocksToDisk(sorted_terms, dictionary,outputFile)\n",
    "    return num_block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BSBIndexConstuction(data):\n",
    "\n",
    "    global global_dictionary, global_block_size\n",
    "\n",
    "    block_n = 0 # numero de bloque actual \n",
    "\n",
    "    f = {} # archivos de bloques\n",
    "    for doc_id, row  in data.iterrows():\n",
    "\n",
    "        token_stream = ParseDocs(row)\n",
    "        num_block = SPIMI_INVERT(token_stream, doc_id, num_block=block_n,  size_per_block=size_per_block, path_block=path_block,)\n",
    "        block_n = num_block\n",
    "        f[num_block] = f'{path_block}block_{num_block}.txt'\n",
    "    \n",
    "    # en caso de que el ultimo bloque no se haya escrito\n",
    "    if global_dictionary:\n",
    "        WriteBlockToDisk(global_dictionary, num_block, path=path_block)\n",
    "        f[num_block] = f'{path_block}block_{num_block}.txt'\n",
    "        num_block += 1\n",
    "        # innecesario pero me sirve para indicar que se acabo y limpiarlo\n",
    "        global_dictionary = SortedDict()\n",
    "        global_block_size = 0\n",
    "\n",
    "\n",
    "    # print(f'Numero de bloques: {num_block}')    \n",
    "\n",
    "    f = MergeBlocks(f)\n",
    "    # shutil.rmtree(path_block)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testeo\n",
    "path = './dataset/'\n",
    "data_path = path + 'spotify_songs_test.csv'\n",
    "\n",
    "global_dictionary = SortedDict()\n",
    "global_block_size = 0\n",
    "\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing block 0 to disk (block size: 41086 bytes)\n",
      "Block 0 written to disk as ./blocks3/block_0.txt\n",
      "Writing block 1 to disk (block size: 41012 bytes)\n",
      "Block 1 written to disk as ./blocks3/block_1.txt\n",
      "Writing block 2 to disk (block size: 41103 bytes)\n",
      "Block 2 written to disk as ./blocks3/block_2.txt\n",
      "Writing block 3 to disk (block size: 41122 bytes)\n",
      "Block 3 written to disk as ./blocks3/block_3.txt\n",
      "Writing block 4 to disk (block size: 41088 bytes)\n",
      "Block 4 written to disk as ./blocks3/block_4.txt\n",
      "Writing block 5 to disk (block size: 41046 bytes)\n",
      "Block 5 written to disk as ./blocks3/block_5.txt\n",
      "Writing block 6 to disk (block size: 41071 bytes)\n",
      "Block 6 written to disk as ./blocks3/block_6.txt\n",
      "Writing block 7 to disk (block size: 41143 bytes)\n",
      "Block 7 written to disk as ./blocks3/block_7.txt\n",
      "Writing block 8 to disk (block size: 41071 bytes)\n",
      "Block 8 written to disk as ./blocks3/block_8.txt\n",
      "Writing block 9 to disk (block size: 41110 bytes)\n",
      "Block 9 written to disk as ./blocks3/block_9.txt\n",
      "Writing block 10 to disk (block size: 41089 bytes)\n",
      "Block 10 written to disk as ./blocks3/block_10.txt\n",
      "Writing block 11 to disk (block size: 41185 bytes)\n",
      "Block 11 written to disk as ./blocks3/block_11.txt\n",
      "Block 12 written to disk as ./blocks3/block_12.txt\n",
      "Block 0 written to ./merged_blocks/block_0.txt\n",
      "Block 1 written to ./merged_blocks/block_1.txt\n",
      "Block 2 written to ./merged_blocks/block_2.txt\n",
      "Block 3 written to ./merged_blocks/block_3.txt\n",
      "Block 4 written to ./merged_blocks/block_4.txt\n",
      "Block 5 written to ./merged_blocks/block_5.txt\n",
      "Block 6 written to ./merged_blocks/block_6.txt\n",
      "Block 7 written to ./merged_blocks/block_7.txt\n",
      "Block 8 written to ./merged_blocks/block_8.txt\n",
      "Block 9 written to ./merged_blocks/block_9.txt\n",
      "Block 10 written to ./merged_blocks/block_10.txt\n",
      "Block 11 written to ./merged_blocks/block_11.txt\n"
     ]
    }
   ],
   "source": [
    "bloques_merged = BSBIndexConstuction(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ram_merge = 1024*1024*1024 # 1gb\n",
    "output_path = './merged_blocks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_block_line(line):\n",
    "    \"\"\"Parsea una línea del archivo de bloque en el formato (término, DF, PostingBlock).\"\"\"\n",
    "    # print(f\"Processing line: {line}\")\n",
    "    term, rest = line.split(\" (DF: \")\n",
    "    df, postings = rest.split(\"): \")\n",
    "    df = int(df)\n",
    "    posting_block = PostingBlock()\n",
    "\n",
    "    postings_list = postings.strip(\"()\").split(\"), (\")\n",
    "    # print(f'postings_list: {postings_list}')\n",
    "    for posting in postings_list:\n",
    "        # Extraer doc_id y tf\n",
    "        doc_id, tf = map(int, posting.strip(\"()\").split(\", \"))\n",
    "        posting_block.add_doc(doc_id, tf)\n",
    "    return term, df, posting_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uso un buffer para controlar la cantidad de datos \n",
    "def dynamic_load_buffer(file, max_memory_per_block, current_memory_usage):\n",
    "    \"\"\"Carga dinámicamente el número de líneas desde un archivo en función de la memoria disponible.\"\"\"\n",
    "    buffer = []\n",
    "    while current_memory_usage < max_memory_per_block:\n",
    "        line = file.readline().strip()\n",
    "        if line:\n",
    "            term, df, posting_block = parse_block_line(line)\n",
    "            buffer.append((term, df, posting_block))\n",
    "            current_memory_usage += sys.getsizeof(term) + sys.getsizeof(df) + sys.getsizeof(posting_block)\n",
    "        else:\n",
    "            break\n",
    "    return buffer, current_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_output_block(block_id, \n",
    "                          output_terms, \n",
    "                          output_folder):\n",
    "    \"\"\"Escribe los términos y postings en un bloque de salida de 4 KB.\"\"\"\n",
    "    output_file = os.path.join(output_folder, f\"block_{block_id}.txt\")\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        for term, (df, posting_block) in output_terms.items():\n",
    "            term_entry = f\"{term} (DF: {df}): \"\n",
    "            postings = []\n",
    "\n",
    "            # Recorre todos los bloques de postings enlazados\n",
    "            current_block = posting_block\n",
    "            while current_block is not None:\n",
    "                postings.extend([f\"({doc}, {tf})\" for doc, tf in current_block.doc_dict.items()])\n",
    "                current_block = current_block.next_block\n",
    "\n",
    "            # Unir todos los postings en una cadena y escribir la entrada completa\n",
    "            term_entry += ', '.join(postings) + \"\\n\"\n",
    "            f.write(term_entry)\n",
    "\n",
    "    print(f\"Block {block_id} written to {output_file}\")\n",
    "    return block_id + 1, output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MergeBlocks(block_files, \n",
    "                output_folder='./merged_blocks/', \n",
    "                block_size_limit=4096, \n",
    "                max_ram_limit=1024*1024*1024):\n",
    "    \n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Abrir archivos de bloques\n",
    "    open_files = {file_id: open(file_path, 'r') for file_id, file_path in block_files.items()}\n",
    "    \n",
    "    # inicializar buffers de entrada\n",
    "    input_buffers = {}\n",
    "    current_ram_usage = 0\n",
    "    # dedico la ram disponible quitando el del bloque de output, que seran las lineas que voy a traer de cada bloque\n",
    "    max_memory_per_block = (max_ram_limit - block_size_limit) // len(block_files) \n",
    "    merged_files = []\n",
    "    \n",
    "    heap = []\n",
    "    for file_id, file in open_files.items():\n",
    "        buffer = []\n",
    "        buffer_size = 0\n",
    "        while buffer_size < max_memory_per_block:\n",
    "            line = file.readline().strip()\n",
    "            if not line:\n",
    "                break  # Si no hay más líneas en el archivo, salir del bucle\n",
    "            term, df, posting_block = parse_block_line(line)\n",
    "            buffer.append((term, df, posting_block))\n",
    "            current_ram_usage += sys.getsizeof(term) + sys.getsizeof(df) + sys.getsizeof(posting_block)\n",
    "        \n",
    "        # Añadir el buffer al diccionario de buffers y cargar sus términos en el heap\n",
    "        input_buffers[file_id] = buffer\n",
    "        \n",
    "        for term, df, posting_block in buffer:\n",
    "            found = False\n",
    "            for i, (existing_term, existing_df, existing_file_id, existing_posting_block) in enumerate(heap):\n",
    "                if existing_term == term:\n",
    "                    existing_df += df\n",
    "                    current_block = existing_posting_block\n",
    "                    while current_block.next_block:\n",
    "                        current_block = current_block.next_block\n",
    "                    if current_block.is_full():\n",
    "                        current_block.next_block = posting_block\n",
    "                    else:\n",
    "                        for doc, tf in posting_block.doc_dict.items():\n",
    "                            if current_block.is_full():\n",
    "                                new_block = PostingBlock()\n",
    "                                current_block.next_block = new_block\n",
    "                                current_block = new_block\n",
    "                            current_block.add_doc(doc, tf)\n",
    "                    heap[i] = (existing_term, existing_df, existing_file_id, existing_posting_block)\n",
    "                    found = True\n",
    "                    break\n",
    "            \n",
    "            if not found:\n",
    "                heapq.heappush(heap, (term, df, file_id, posting_block))\n",
    "\n",
    "\n",
    "    # print(\"Initial heap content:\")\n",
    "    # for item in heap:\n",
    "    #     print(item)\n",
    "\n",
    "    block_id = 0\n",
    "    output_terms = {}\n",
    "    current_output_size = 0\n",
    "\n",
    "    while heap:\n",
    "        # Extraer el término mínimo del heap\n",
    "        term, df, file_id, posting_block = heapq.heappop(heap)\n",
    "\n",
    "        # Fusionar postings si el término ya existe en output_terms\n",
    "        if term in output_terms:\n",
    "            existing_df, existing_posting_block = output_terms[term]\n",
    "            output_terms[term] = (existing_df + df, existing_posting_block)\n",
    "            current_block = existing_posting_block\n",
    "            while current_block.next_block:\n",
    "                current_block = current_block.next_block\n",
    "            if current_block.is_full():\n",
    "                current_block.next_block = posting_block\n",
    "            else:\n",
    "                for doc, tf in posting_block.doc_dict.items():\n",
    "                    if current_block.is_full():\n",
    "                        new_block = PostingBlock()\n",
    "                        current_block.next_block = new_block\n",
    "                        current_block = new_block\n",
    "                    current_block.add_doc(doc, tf)\n",
    "            \n",
    "            # Solo sumar el tamaño de los nuevos postings agregados\n",
    "            additional_postings_size = len(', '.join([f\"({doc}, {tf})\" for doc, tf in posting_block.doc_dict.items()]))\n",
    "            current_output_size += additional_postings_size\n",
    "        else:\n",
    "            output_terms[term] = (df, posting_block)\n",
    "\n",
    "            # Calcular el tamaño en memoria del término y postings\n",
    "            # term_size = sys.getsizeof(term) + sys.getsizeof(df) + sys.agetsizeof(posting_block)\n",
    "            postings_str = ', '.join([f\"({doc}, {tf})\" for doc, tf in posting_block.doc_dict.items()])\n",
    "            term_entry = f\"{term} (DF: {df}): {postings_str}\\n\"\n",
    "            term_size = len(term_entry)\n",
    "            current_output_size += term_size\n",
    "\n",
    "        # Escribir el bloque de salida cuando se alcanza el límite\n",
    "        if current_output_size > block_size_limit:\n",
    "            # print(f'Tamaño hasta ahora: {current_output_size}')\n",
    "            block_id, block_file = write_to_output_block(block_id, output_terms, output_folder)\n",
    "            merged_files.append(block_file)\n",
    "            output_terms.clear()\n",
    "            current_output_size = 0\n",
    "        \n",
    "        # Recargar el buffer si está vacío\n",
    "        if not input_buffers[file_id]:\n",
    "            # Cargar nuevas líneas hasta llenar el buffer o alcanzar el límite de RAM\n",
    "            buffer = []\n",
    "            buffer_size = 0\n",
    "            while buffer_size < max_memory_per_block:\n",
    "                line = open_files[file_id].readline().strip()\n",
    "                if not line:\n",
    "                    break  # No hay más líneas en el archivo\n",
    "                term, df, posting_block = parse_block_line(line)\n",
    "                buffer.append((term, df, posting_block))\n",
    "                buffer_size += sys.getsizeof(term) + sys.getsizeof(df) + sys.getsizeof(posting_block)\n",
    "            \n",
    "            input_buffers[file_id] = buffer  # Actualizar el buffer\n",
    "\n",
    "        # Agregar el siguiente término al heap\n",
    "        if input_buffers[file_id]:\n",
    "            next_term, next_df, next_posting_block = input_buffers[file_id].pop(0)\n",
    "            heapq.heappush(heap, (next_term, next_df, file_id, next_posting_block))\n",
    "\n",
    "    # Escribir el último bloque de salida si hay términos restantes\n",
    "    if output_terms:\n",
    "        block_id, block_file = write_to_output_block(block_id, output_terms, output_folder)\n",
    "        merged_files.append(block_file)\n",
    "\n",
    "    for file in open_files.values():\n",
    "        file.close()\n",
    "\n",
    "    return merged_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: './blocks3/block_0.txt', 1: './blocks3/block_1.txt', 2: './blocks3/block_2.txt', 4: './blocks3/block_4.txt', 6: './blocks3/block_6.txt', 7: './blocks3/block_7.txt', 8: './blocks3/block_8.txt', 9: './blocks3/block_9.txt', 10: './blocks3/block_10.txt', 11: './blocks3/block_11.txt', 12: './blocks3/block_12.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(bloques_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0 written to ./merged_index2/merged_block_0.txt\n",
      "Block 1 written to ./merged_index2/merged_block_1.txt\n",
      "Block 2 written to ./merged_index2/merged_block_2.txt\n",
      "Block 3 written to ./merged_index2/merged_block_3.txt\n",
      "Block 4 written to ./merged_index2/merged_block_4.txt\n",
      "Block 5 written to ./merged_index2/merged_block_5.txt\n",
      "Block 6 written to ./merged_index2/merged_block_6.txt\n",
      "Block 7 written to ./merged_index2/merged_block_7.txt\n",
      "Block 8 written to ./merged_index2/merged_block_8.txt\n"
     ]
    }
   ],
   "source": [
    "output_path = './merged_index2/'\n",
    "block_limit_size = 1024*5.9\n",
    "max_ram_limit = 1024*1024*1024\n",
    "\n",
    "MergeBlocks(bloques_merged, output_path, block_limit_size, max_ram_limit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
