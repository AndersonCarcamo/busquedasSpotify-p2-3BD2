{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T22:48:49.716018Z",
     "start_time": "2024-11-28T22:48:40.025339Z"
    }
   },
   "source": [
    "from spimi import SPIMI\n",
    "import pandas as pd\n",
    "\n",
    "# testeo\n",
    "path = './dataset/'\n",
    "data_path = path + 'spotify_songs.csv'\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "\n",
    "data1000 = data.head(1000)\n",
    "data5000 = data.head(5000)\n",
    "data10000 = data.head(10000)\n",
    "data18000 = data"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Anderson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T22:48:49.786661Z",
     "start_time": "2024-11-28T22:48:49.748575Z"
    }
   },
   "source": [
    "doc_metadata_map = data1000.to_dict(\"index\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T22:49:26.741740Z",
     "start_time": "2024-11-28T22:49:26.735741Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import pandas as pd\n",
    "import bisect"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T22:49:59.663606Z",
     "start_time": "2024-11-28T22:49:59.231541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Anderson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T22:50:19.012429Z",
     "start_time": "2024-11-28T22:50:18.988936Z"
    }
   },
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "class CosineSimilaritySearch:\n",
    "    def __init__(self, block_folder, num_docs, lang='en'):\n",
    "        self.block_folder = block_folder\n",
    "        self.num_docs = num_docs\n",
    "        self.language_map = {\n",
    "            'es': 'spanish',\n",
    "            'en': 'english',\n",
    "            'fr': 'french',\n",
    "            'de': 'german',\n",
    "            'it': 'italian'\n",
    "        }\n",
    "        self.lang = lang\n",
    "        self.stop_words = set(stopwords.words(self.language_map.get(lang, 'english')))\n",
    "        self.stemmer = SnowballStemmer(language=self.language_map.get(lang, 'english'))\n",
    "\n",
    "    def preprocess_query(self, query):\n",
    "        query = query.lower()\n",
    "        tokens = [self.stemmer.stem(word) for word in query.split() if word not in self.stop_words]\n",
    "        return tokens\n",
    "\n",
    "    def parse_line(self, line):\n",
    "        \"\"\"\n",
    "        Parsear una línea del bloque en formato: \"term (DF: x): (doc_id, tf), ...\"\n",
    "        \"\"\"\n",
    "        line = line.strip()  # Eliminar saltos de línea y espacios extra\n",
    "        try:\n",
    "            term, rest = line.split(\" (DF: \")\n",
    "            df, postings = rest.split(\"): \")\n",
    "            df = int(df)\n",
    "            postings_list = [\n",
    "                tuple(map(int, posting.strip(\"()\").split(\", \"))) for posting in postings.split(\"), (\")\n",
    "            ]\n",
    "            return term, df, postings_list\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Error procesando la línea: {line}. Detalles: {str(e)}\")\n",
    "\n",
    "    def search_term_in_block(self, token, block_path):\n",
    "        \"\"\"\n",
    "        Busca un término dentro de un bloque específico utilizando búsqueda binaria.\n",
    "        \"\"\"\n",
    "        with open(block_path, \"r\") as block_file:\n",
    "            lines = [line.strip() for line in block_file.readlines()]\n",
    "\n",
    "        low, high = 0, len(lines) - 1\n",
    "        while low <= high:\n",
    "            mid = (low + high) // 2\n",
    "            line = lines[mid]\n",
    "            term, df, postings_list = self.parse_line(line)\n",
    "\n",
    "            if token == term:\n",
    "                return postings_list, df\n",
    "            elif token < term:\n",
    "                high = mid - 1\n",
    "            else:\n",
    "                low = mid + 1\n",
    "        return None, 0\n",
    "\n",
    "    def search_term(self, token):\n",
    "        \"\"\"\n",
    "        Encuentra el bloque donde puede estar el término y busca dentro de él.\n",
    "        \"\"\"\n",
    "        def natural_sort_key(s):\n",
    "            return [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\\d+)', s)]\n",
    "\n",
    "        block_files = sorted(os.listdir(self.block_folder), key=natural_sort_key)\n",
    "        # print(block_files)\n",
    "        low, high = 0, len(block_files) - 1\n",
    "        # print(f'low: {low}, high: {high}')\n",
    "        while low <= high:\n",
    "            mid = (low + high) // 2\n",
    "            # print('mid:', mid)\n",
    "            block_path = os.path.join(self.block_folder, block_files[mid])\n",
    "            # print(f'Block_path: {block_path}')\n",
    "            with open(block_path, \"r\") as block_file:\n",
    "                lines = [line.strip() for line in block_file.readlines()]\n",
    "                # print(f\"Línea 1 del bloque: {lines[0]}\")\n",
    "                # print(f\"Last line del bloque: {lines[-1]}\")\n",
    "                first_line = lines[0]\n",
    "                last_line = lines[-1]\n",
    "\n",
    "            first_term, _, _ = self.parse_line(first_line)\n",
    "            last_term, _, _ = self.parse_line(last_line)\n",
    "\n",
    "            if first_term <= token <= last_term:\n",
    "                return self.search_term_in_block(token, block_path)\n",
    "            elif token < first_term:\n",
    "                high = mid - 1\n",
    "            else:\n",
    "                low = mid + 1\n",
    "        return None, 0\n",
    "\n",
    "    def cosine_similarity(self, query, topk):\n",
    "        \"\"\"\n",
    "        Calcula la similitud de coseno entre la consulta y los documentos.\n",
    "        \"\"\"\n",
    "        scores = defaultdict(float)\n",
    "        query_terms = self.preprocess_query(query)\n",
    "        print(query_terms)\n",
    "        norm_query = 0\n",
    "        df_dict = {}\n",
    "        tf_query = defaultdict(int)\n",
    "        global_tf_idf_squares = defaultdict(float)\n",
    "\n",
    "        for token in query_terms:\n",
    "            tf_query[token] += 1\n",
    "\n",
    "        print(f\"TF de la consulta: {tf_query}\")\n",
    "\n",
    "        for token, tf in tf_query.items():\n",
    "            postings_list, df = self.search_term(token)\n",
    "            # print(f\"Postings list: {postings_list} con DF: {df} para término: {token}\")\n",
    "            if postings_list:\n",
    "                df_dict[token] = df\n",
    "                idf = np.log10(self.num_docs / df)\n",
    "                tf_weight_query = np.log10(1 + tf)\n",
    "                wt_query = tf_weight_query * idf\n",
    "\n",
    "                norm_query += np.square(wt_query)\n",
    "\n",
    "                for doc_id, tf_doc in postings_list:\n",
    "                    tf_weight_doc = np.log10(tf_doc + 1)\n",
    "                    wt_doc = tf_weight_doc * idf\n",
    "                    scores[doc_id] += wt_query * wt_doc\n",
    "                    global_tf_idf_squares[doc_id] += wt_doc ** 2\n",
    "\n",
    "        norm_query = np.sqrt(norm_query)\n",
    "        norm_global = np.sqrt(sum(global_tf_idf_squares.values()))\n",
    "\n",
    "        for doc_id, score in scores.items():\n",
    "            norm_doc = np.sqrt(global_tf_idf_squares[doc_id])\n",
    "            if norm_query != 0 and norm_doc != 0 and norm_global != 0:\n",
    "                scores[doc_id] = score / (norm_query * norm_global)\n",
    "            else:\n",
    "                scores[doc_id] = 0\n",
    "\n",
    "        topk_docs = sorted(scores.items(), key=lambda item: item[1], reverse=True)[:topk]\n",
    "        \n",
    "        for doc_id, similarity in topk_docs:\n",
    "            # Recuperar información del DataFrame basado en el índice (doc_id)\n",
    "            metadata = doc_metadata_map.get(doc_id, {})\n",
    "            track_name = metadata.get(\"track_name\", \"Unknown\")\n",
    "            track_artist = metadata.get(\"track_artist\", \"Unknown\")\n",
    "            album_name = metadata.get(\"track_album_name\", \"Unknown\")\n",
    "            release_date = metadata.get(\"track_album_release_date\", \"Unknown\")\n",
    "            print(f\"Documento: {doc_id}, Similitud: {similarity:.4f}, \"\n",
    "                f\"Título: {track_name}, Artista: {track_artist}, \"\n",
    "                f\"Álbum: {album_name}, Fecha de lanzamiento: {release_date}\")\n",
    "\n",
    "\n",
    "\n",
    "        return topk_docs"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T22:51:14.382514Z",
     "start_time": "2024-11-28T22:51:14.370107Z"
    }
   },
   "source": [
    "block_folder = './blocks1000/'\n",
    "num_docs = 1000\n",
    "\n",
    "search_engine = CosineSimilaritySearch(block_folder, num_docs, lang='es')\n",
    "\n",
    "query = \"Me enamore\"\n",
    "topk_results = search_engine.cosine_similarity(query, topk=100)\n",
    "\n",
    "# print(\"Top K documentos más similares:\")\n",
    "# for doc_id, similarity in topk_results:\n",
    "#     print(f\"Documento: {doc_id}, Similitud: {similarity}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enamor']\n",
      "TF de la consulta: defaultdict(<class 'int'>, {'enamor': 1})\n",
      "Documento: 19, Similitud: 0.4653, Título: Me Enamoré, Artista: Jay Wheeler, Álbum: Platónico, Fecha de lanzamiento: 2019-11-28\n",
      "Documento: 366, Similitud: 0.4382, Título: More, Artista: Zion, Álbum: La Fórmula, Fecha de lanzamiento: 2012-08-20\n",
      "Documento: 481, Similitud: 0.3790, Título: Sola, Artista: Luis Fonsi, Álbum: Sola, Fecha de lanzamiento: 2019-01-23\n",
      "Documento: 173, Similitud: 0.3075, Título: Te Vi, Artista: Piso 21, Álbum: Te Vi, Fecha de lanzamiento: 2018-12-14\n",
      "Documento: 269, Similitud: 0.2832, Título: 22, Artista: TINI, Álbum: 22, Fecha de lanzamiento: 2019-05-03\n",
      "Documento: 477, Similitud: 0.2832, Título: Vete, Artista: Khea, Álbum: Vete, Fecha de lanzamiento: 2017-12-21\n",
      "Documento: 934, Similitud: 0.2832, Título: Nunca Es Suficiente, Artista: Los Angeles Azules, Álbum: Esto Sí Es Cumbia, Fecha de lanzamiento: 2018-06-08\n",
      "Documento: 580, Similitud: 0.2544, Título: Mi Medicina, Artista: CNCO, Álbum: CNCO, Fecha de lanzamiento: 2018-04-06\n",
      "Documento: 14, Similitud: 0.1095, Título: Latina (feat. Maluma), Artista: Reykon, Álbum: Latina (feat. Maluma), Fecha de lanzamiento: 2019-05-31\n",
      "Documento: 417, Similitud: 0.1095, Título: A las Cosas por Su Nombre, Artista: Violadores Del Verso, Álbum: Vivir para Contarlo, Fecha de lanzamiento: 2006\n",
      "Documento: 918, Similitud: 0.1095, Título: Mi soledad y yo, Artista: Alejandro Sanz, Álbum: Grandes exitos 1991-1996, Fecha de lanzamiento: 2004-09-23\n",
      "Documento: 996, Similitud: 0.1095, Título: Te Guste, Artista: Jennifer Lopez, Álbum: Te Guste, Fecha de lanzamiento: 2018-11-09\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
