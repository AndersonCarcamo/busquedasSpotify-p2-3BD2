{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ambar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from spimi import SPIMI\n",
    "import pandas as pd\n",
    "\n",
    "# testeo\n",
    "path = './dataset/'\n",
    "data_path = path + 'spotify_songs.csv'\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "\n",
    "data1000 = data.head(1000)\n",
    "data5000 = data.head(5000)\n",
    "data10000 = data.head(10000)\n",
    "data18000 = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_metadata_map = data1000.to_dict(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import pandas as pd\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "class CosineSimilaritySearch:\n",
    "    def __init__(self, block_folder, num_docs, lang='en'):\n",
    "        self.block_folder = block_folder\n",
    "        self.num_docs = num_docs\n",
    "        self.language_map = {\n",
    "            'es': 'spanish',\n",
    "            'en': 'english',\n",
    "            'fr': 'french',\n",
    "            'de': 'german',\n",
    "            'it': 'italian'\n",
    "        }\n",
    "        self.lang = lang\n",
    "        self.stop_words = set(stopwords.words(self.language_map.get(lang, 'english')))\n",
    "        self.stemmer = SnowballStemmer(language=self.language_map.get(lang, 'english'))\n",
    "\n",
    "    def preprocess_query(self, query):\n",
    "        query = query.lower()\n",
    "        tokens = [self.stemmer.stem(word) for word in query.split() if word not in self.stop_words]\n",
    "        return tokens\n",
    "\n",
    "    def parse_line(self, line):\n",
    "        \"\"\"\n",
    "        Parsear una línea del bloque en formato: \"term (DF: x): (doc_id, tf), ...\"\n",
    "        \"\"\"\n",
    "        line = line.strip()  # Eliminar saltos de línea y espacios extra\n",
    "        try:\n",
    "            term, rest = line.split(\" (DF: \")\n",
    "            df, postings = rest.split(\"): \")\n",
    "            df = int(df)\n",
    "            postings_list = [\n",
    "                tuple(map(int, posting.strip(\"()\").split(\", \"))) for posting in postings.split(\"), (\")\n",
    "            ]\n",
    "            return term, df, postings_list\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Error procesando la línea: {line}. Detalles: {str(e)}\")\n",
    "\n",
    "    def search_term_in_block(self, token, block_path):\n",
    "        \"\"\"\n",
    "        Busca un término dentro de un bloque específico utilizando búsqueda binaria.\n",
    "        \"\"\"\n",
    "        with open(block_path, \"r\") as block_file:\n",
    "            lines = [line.strip() for line in block_file.readlines()]\n",
    "\n",
    "        low, high = 0, len(lines) - 1\n",
    "        while low <= high:\n",
    "            mid = (low + high) // 2\n",
    "            line = lines[mid]\n",
    "            term, df, postings_list = self.parse_line(line)\n",
    "\n",
    "            if token == term:\n",
    "                return postings_list, df\n",
    "            elif token < term:\n",
    "                high = mid - 1\n",
    "            else:\n",
    "                low = mid + 1\n",
    "        return None, 0\n",
    "\n",
    "    def search_term(self, token):\n",
    "        \"\"\"\n",
    "        Encuentra el bloque donde puede estar el término y busca dentro de él.\n",
    "        \"\"\"\n",
    "        def natural_sort_key(s):\n",
    "            return [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\\d+)', s)]\n",
    "\n",
    "        block_files = sorted(os.listdir(self.block_folder), key=natural_sort_key)\n",
    "        # print(block_files)\n",
    "        low, high = 0, len(block_files) - 1\n",
    "        # print(f'low: {low}, high: {high}')\n",
    "        while low <= high:\n",
    "            mid = (low + high) // 2\n",
    "            # print('mid:', mid)\n",
    "            block_path = os.path.join(self.block_folder, block_files[mid])\n",
    "            # print(f'Block_path: {block_path}')\n",
    "            with open(block_path, \"r\") as block_file:\n",
    "                lines = [line.strip() for line in block_file.readlines()]\n",
    "                # print(f\"Línea 1 del bloque: {lines[0]}\")\n",
    "                # print(f\"Last line del bloque: {lines[-1]}\")\n",
    "                first_line = lines[0]\n",
    "                last_line = lines[-1]\n",
    "\n",
    "            first_term, _, _ = self.parse_line(first_line)\n",
    "            last_term, _, _ = self.parse_line(last_line)\n",
    "\n",
    "            if first_term <= token <= last_term:\n",
    "                return self.search_term_in_block(token, block_path)\n",
    "            elif token < first_term:\n",
    "                high = mid - 1\n",
    "            else:\n",
    "                low = mid + 1\n",
    "        return None, 0\n",
    "\n",
    "    def cosine_similarity(self, query, topk):\n",
    "        \"\"\"\n",
    "        Calcula la similitud de coseno entre la consulta y los documentos.\n",
    "        \"\"\"\n",
    "        scores = defaultdict(float)\n",
    "        query_terms = self.preprocess_query(query)\n",
    "        print(query_terms)\n",
    "        norm_query = 0\n",
    "        df_dict = {}\n",
    "        tf_query = defaultdict(int)\n",
    "        global_tf_idf_squares = defaultdict(float)\n",
    "\n",
    "        for token in query_terms:\n",
    "            tf_query[token] += 1\n",
    "\n",
    "        print(f\"TF de la consulta: {tf_query}\")\n",
    "\n",
    "        for token, tf in tf_query.items():\n",
    "            postings_list, df = self.search_term(token)\n",
    "            print(f\"Postings list: {postings_list} con DF: {df} para término: {token}\")\n",
    "            if postings_list:\n",
    "                df_dict[token] = df\n",
    "                idf = np.log10(self.num_docs / df)\n",
    "                tf_weight_query = np.log10(1 + tf)\n",
    "                wt_query = tf_weight_query * idf\n",
    "\n",
    "                norm_query += np.square(wt_query)\n",
    "\n",
    "                for doc_id, tf_doc in postings_list:\n",
    "                    tf_weight_doc = np.log10(tf_doc + 1)\n",
    "                    wt_doc = tf_weight_doc * idf\n",
    "                    scores[doc_id] += wt_query * wt_doc\n",
    "                    global_tf_idf_squares[doc_id] += wt_doc ** 2\n",
    "\n",
    "        norm_query = np.sqrt(norm_query)\n",
    "        norm_global = np.sqrt(sum(global_tf_idf_squares.values()))\n",
    "\n",
    "        for doc_id, score in scores.items():\n",
    "            norm_doc = np.sqrt(global_tf_idf_squares[doc_id])\n",
    "            if norm_query != 0 and norm_doc != 0 and norm_global != 0:\n",
    "                scores[doc_id] = score / (norm_query * norm_global)\n",
    "            else:\n",
    "                scores[doc_id] = 0\n",
    "\n",
    "        topk_docs = sorted(scores.items(), key=lambda item: item[1], reverse=True)[:topk]\n",
    "        \n",
    "        for doc_id, similarity in topk_docs:\n",
    "            # Recuperar información del DataFrame basado en el índice (doc_id)\n",
    "            metadata = doc_metadata_map.get(doc_id, {})\n",
    "            track_name = metadata.get(\"track_name\", \"Unknown\")\n",
    "            track_artist = metadata.get(\"track_artist\", \"Unknown\")\n",
    "            album_name = metadata.get(\"track_album_name\", \"Unknown\")\n",
    "            release_date = metadata.get(\"track_album_release_date\", \"Unknown\")\n",
    "            print(f\"Documento: {doc_id}, Similitud: {similarity:.4f}, \"\n",
    "                f\"Título: {track_name}, Artista: {track_artist}, \"\n",
    "                f\"Álbum: {album_name}, Fecha de lanzamiento: {release_date}\")\n",
    "\n",
    "\n",
    "\n",
    "        return topk_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['let', 'roll', 'it']\n",
      "TF de la consulta: defaultdict(<class 'int'>, {'let': 1, 'roll': 1, 'it': 1})\n",
      "Postings list: [(2, 1), (4, 1), (5, 1), (8, 8), (11, 1), (13, 6), (15, 13), (22, 3), (24, 6), (27, 1), (36, 26), (37, 7), (40, 1), (47, 1), (48, 2), (54, 2), (59, 2), (62, 1), (66, 1), (68, 3), (69, 2), (74, 2), (78, 1), (79, 2), (80, 1), (85, 3), (88, 1), (89, 4), (98, 4), (100, 1), (104, 3), (106, 1), (110, 3), (112, 3), (113, 9), (114, 2), (116, 2), (118, 1), (119, 1), (120, 1), (121, 2), (126, 3), (128, 2), (129, 2), (131, 26), (140, 2), (141, 4), (142, 1), (148, 2), (150, 8), (153, 24), (154, 1), (156, 7), (162, 8), (166, 1), (169, 4), (171, 2), (172, 1), (174, 1), (176, 18), (177, 1), (178, 7), (179, 1), (181, 2), (183, 1), (188, 4), (190, 4), (191, 2), (193, 16), (194, 1), (197, 3), (200, 3), (201, 1), (204, 1), (205, 3), (207, 1), (208, 23), (209, 7), (210, 3), (215, 1), (219, 2), (220, 1), (223, 3), (225, 1), (226, 1), (228, 1), (231, 4), (232, 2), (235, 1), (236, 2), (237, 1), (238, 2), (242, 2), (243, 1), (255, 9), (257, 7), (258, 1), (262, 2), (264, 1), (265, 2), (268, 3), (271, 4), (272, 3), (273, 8), (274, 2), (275, 4), (278, 2), (282, 18), (284, 2), (287, 2), (288, 6), (289, 1), (294, 2), (298, 7), (301, 2), (303, 4), (304, 1), (308, 2), (310, 3), (314, 4), (319, 32), (320, 4), (325, 17), (327, 1), (329, 12), (331, 1), (334, 2), (336, 1), (337, 2), (339, 2), (340, 8), (350, 1), (352, 2), (356, 2), (357, 1), (358, 1), (361, 1), (367, 4), (372, 6), (373, 1), (375, 4), (376, 1), (380, 4), (381, 3), (384, 3), (387, 2), (391, 4), (394, 5), (404, 12), (405, 11), (407, 2), (414, 1), (418, 1), (419, 3), (422, 4), (423, 1), (431, 6), (432, 2), (435, 1), (440, 5), (442, 9), (446, 10), (447, 4), (449, 3), (451, 1), (452, 3), (455, 4), (456, 1), (457, 4), (458, 16), (461, 18), (464, 16), (469, 1), (470, 1), (471, 4), (473, 3), (474, 2), (476, 1), (478, 4), (479, 9), (482, 2), (485, 8), (486, 1), (487, 2), (488, 1), (490, 1), (494, 2), (495, 4), (496, 3), (501, 1), (502, 3), (507, 4), (508, 8), (511, 3), (514, 28), (517, 4), (521, 1), (525, 14), (526, 4), (530, 1), (531, 4), (535, 10), (537, 2), (541, 2), (545, 1), (548, 1), (550, 3), (554, 5), (556, 17), (558, 1), (566, 1), (568, 1), (570, 6), (572, 1), (573, 1), (574, 1), (581, 1), (583, 1), (584, 42), (585, 4), (586, 5), (590, 6), (591, 2), (593, 3), (595, 1), (597, 4), (602, 4), (603, 4), (606, 1), (607, 2), (608, 1), (610, 49), (611, 2), (612, 1), (613, 1), (616, 3), (618, 2), (619, 1), (622, 1), (627, 10), (633, 7), (634, 1), (646, 1), (647, 2), (650, 1), (654, 1), (655, 11), (657, 4), (660, 2), (662, 2), (663, 1), (669, 1), (671, 2), (672, 7), (684, 3), (696, 1), (697, 2), (701, 17), (709, 3), (710, 2), (714, 5), (719, 2), (720, 1), (722, 3), (724, 1), (731, 3), (732, 17), (734, 12), (737, 2), (738, 6), (740, 2), (745, 43), (749, 2), (753, 4), (754, 2), (755, 4), (758, 2), (766, 2), (767, 6), (769, 2), (772, 2), (775, 2), (779, 1), (781, 7), (783, 1), (786, 4), (789, 5), (790, 2), (792, 2), (796, 1), (800, 1), (802, 2), (803, 3), (806, 2), (811, 1), (814, 1), (816, 1), (818, 2), (820, 1), (824, 2), (825, 1), (826, 1), (828, 1), (832, 10), (833, 2), (834, 5), (838, 1), (841, 1), (846, 1), (847, 20), (852, 1), (856, 3), (859, 2), (860, 3), (863, 1), (866, 4), (867, 3), (870, 1), (874, 1), (879, 5), (882, 2), (884, 1), (886, 1), (888, 1), (895, 1), (896, 2), (898, 5), (899, 3), (900, 2), (904, 4), (906, 2), (909, 2), (914, 1), (920, 1), (925, 4), (926, 1), (927, 1), (928, 1), (936, 3), (937, 49), (939, 2), (944, 3), (946, 4), (948, 2), (952, 3), (955, 2), (958, 4), (959, 3), (966, 1), (968, 33), (970, 4), (973, 1), (974, 2), (975, 2), (977, 7), (978, 2), (979, 9), (983, 1), (985, 6), (988, 2), (990, 1), (993, 1), (997, 1), (998, 1), (999, 2)] con DF: 365 para término: let\n",
      "Postings list: [(15, 1), (26, 3), (51, 1), (65, 1), (68, 3), (76, 1), (104, 1), (110, 1), (148, 1), (169, 58), (172, 1), (220, 1), (222, 6), (223, 1), (278, 8), (285, 8), (287, 1), (288, 1), (306, 1), (313, 2), (324, 1), (325, 17), (327, 1), (339, 3), (389, 1), (392, 2), (393, 1), (400, 2), (407, 2), (468, 6), (474, 2), (510, 2), (522, 1), (532, 2), (535, 2), (537, 4), (541, 1), (546, 1), (563, 1), (574, 1), (583, 2), (586, 1), (598, 1), (606, 4), (618, 2), (625, 1), (645, 1), (646, 1), (650, 1), (660, 1), (673, 2), (684, 1), (692, 2), (714, 2), (726, 4), (735, 2), (759, 2), (775, 4), (789, 1), (800, 15), (802, 1), (807, 6), (818, 1), (819, 13), (820, 1), (823, 1), (857, 1), (868, 1), (892, 2), (893, 1), (896, 1), (952, 2), (966, 1), (978, 16), (985, 3), (988, 1), (993, 1)] con DF: 77 para término: roll\n",
      "Postings list: [(227, 4), (858, 1)] con DF: 2 para término: it\n",
      "Documento: 227, Similitud: 0.2165, Título: Safari, Artista: J Balvin, Álbum: Energía, Fecha de lanzamiento: 2016-06-24\n",
      "Documento: 169, Similitud: 0.0990, Título: Roll With Me (feat. Shungudzo) - Friend Within Remix, Artista: Bantu, Álbum: Roll With Me (feat. Shungudzo) [Friend Within Remix], Fecha de lanzamiento: 2018-12-14\n",
      "Documento: 858, Similitud: 0.0932, Título: Wild In Saint Antoine, Artista: Gordo Sarkasmus, Álbum: Herejías, Fecha de lanzamiento: 2015-03-19\n",
      "Documento: 325, Similitud: 0.0764, Título: Let Me Roll It - Remastered 2010, Artista: Paul McCartney, Álbum: Band On The Run (Standard), Fecha de lanzamiento: 1973-12-05\n",
      "Documento: 978, Similitud: 0.0687, Título: Cruise, Artista: Florida Georgia Line, Álbum: Here's To The Good Times, Fecha de lanzamiento: 2012-01-01\n"
     ]
    }
   ],
   "source": [
    "block_folder = './blocks1000/'\n",
    "num_docs = 1000\n",
    "\n",
    "search_engine = CosineSimilaritySearch(block_folder, num_docs, lang='es')\n",
    "\n",
    "query = \"Let Me Roll It\"\n",
    "topk_results = search_engine.cosine_similarity(query, topk=5)\n",
    "\n",
    "# print(\"Top K documentos más similares:\")\n",
    "# for doc_id, similarity in topk_results:\n",
    "#     print(f\"Documento: {doc_id}, Similitud: {similarity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
